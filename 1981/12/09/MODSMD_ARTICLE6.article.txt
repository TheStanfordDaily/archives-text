# Course evaluations can affect Prof. tenure
## 
### Tim Lundergan 
Faculty evaluation forms used by
the ASSU Course Guide — which ask
students to rate their professors' use
of course materials, accessibility out
side of class, interest in student opin
ions and other qualities — are also
used to make decisions on faculty
promotions and tenure.
But the decentralized way in which
the survey is conducted means that
no one is responsible for checking
that the survey measures what it is
supposed to or that people under
stand its proper uses.
Students often do not realize the
survey is used for purposes other
than the Course Guide. Many do not
read the statement of purpose in the
Course Guide, either. And more im
portantly, no notice of the survey's
purpose appears on the question
naire itself.
According to a political science
professor involved in survey research
here, not informing respondents of
the purpose of a questionnaire con
stitutes a violation of the "rules of the

game" of opinion research.
Chris Cramer, who heads this
year's ASSU course guide program,
agrees that both students and pro
fessors have misunderstood the na
ture of the questionnaires. He says
the a course guide intern is working
on a program to explain the survey's
purposes to freshmen beginning
next term.
"Most students don't realize
they're a quality control on a pro
fessor," he said.
The ASSU does not have primary
responsibility for the survey. The
University sends course evaluation
forms to professors, who then give
them to their students. The com
pleted forms are sent to the Office of
Academic Standing.
According to Jim Coleman of the
Office of Academic Standing, the
data is processed through a scanning
machine and the forms are sent back
to individual professors. One month
later, a report which forms the basis
of the ASSU Course Guide is printed.
Professors, whose participation is

voluntary but strongly encouraged
by the Faculty Senate, receive a copy
of this report. Departments can also
receive a copy.
Departments are free to use or ig
nore these reports as they wish. One
professor worries that some depart
ments will "appeal to the magic of
numbers" and use the hard statistics
the questionnaires provide without
considering if these numbers in fact
measure what they are supposed to
measure.
"When the system was set up, it
wasn't set up for the different func
tions" it is now used for, Coleman
said. "Any use beyond feedback was
not originally considered."
Hiring and firing decisions now
could be partially based on questions
that are vague, measure popularity
rather than skill, or do not take differ
ing circumstances into account.
For instance, Coleman said, the
system currently used does not ex
amine whether new professors score
lower than experienced professors,
whether required courses are less

popular than electives, whether
courses in the hard sciences have dif
ferent scores than those in the hu
manities, or if teachers who are un
der consideration for tenure receive
higher scores from sympathetic stu
dents who know student evaluations
are a factor in the decision.
Such rigorous evaluations are pos
sible, Coleman said, but "we're just
beginning to get the material to do
it."
His office now has data from the
three and one quarter years in which
the evaluation forms have been used
to provide a data base from which
such comparisons could be drawn.
According to Cramer, "the Univer
sity has shunted the evaluations onto
the Office of Academic Standing,"
which already has too much work to
do.
The survey process has grown
beyond its original function without
anyone overseeing its growth. No
comprehensive program for inform
ing students and faculty of the sur
vey's functions exists.
Faculty evaluation forms used by
the ASSU Course Guide — which ask
students to rate their professors' use
of course materials, accessibility out
side of class, interest in student opin
ions and other qualities — are also
used to make decisions on faculty
promotions and tenure.
But the decentralized way in which
the survey is conducted means that
no one is responsible for checking
that the survey measures what it is
supposed to or that people under
stand its proper uses.
Students often do not realize the
survey is used for purposes other
than the Course Guide. Many do not
read the statement of purpose in the
Course Guide, either. And more im
portantly, no notice of the survey's
purpose appears on the question
naire itself.
According to a political science
professor involved in survey research
here, not informing respondents of
the purpose of a questionnaire con
stitutes a violation of the "rules of the

game" of opinion research.
Chris Cramer, who heads this
year's ASSU course guide program,
agrees that both students and pro
fessors have misunderstood the na
ture of the questionnaires. He says
the a course guide intern is working
on a program to explain the survey's
purposes to freshmen beginning
next term.
"Most students don't realize
they're a quality control on a pro
fessor," he said.
The ASSU does not have primary
responsibility for the survey. The
University sends course evaluation
forms to professors, who then give
them to their students. The com
pleted forms are sent to the Office of
Academic Standing.
According to Jim Coleman of the
Office of Academic Standing, the
data is processed through a scanning
machine and the forms are sent back
to individual professors. One month
later, a report which forms the basis
of the ASSU Course Guide is printed.
Professors, whose participation is

voluntary but strongly encouraged
by the Faculty Senate, receive a copy
of this report. Departments can also
receive a copy.
Departments are free to use or ig
nore these reports as they wish. One
professor worries that some depart
ments will "appeal to the magic of
numbers" and use the hard statistics
the questionnaires provide without
considering if these numbers in fact
measure what they are supposed to
measure.
"When the system was set up, it
wasn't set up for the different func
tions" it is now used for, Coleman
said. "Any use beyond feedback was
not originally considered."
Hiring and firing decisions now
could be partially based on questions
that are vague, measure popularity
rather than skill, or do not take differ
ing circumstances into account.
For instance, Coleman said, the
system currently used does not ex
amine whether new professors score
lower than experienced professors,
whether required courses are less

popular than electives, whether
courses in the hard sciences have dif
ferent scores than those in the hu
manities, or if teachers who are un
der consideration for tenure receive
higher scores from sympathetic stu
dents who know student evaluations
are a factor in the decision.
Such rigorous evaluations are pos
sible, Coleman said, but "we're just
beginning to get the material to do
it."
His office now has data from the
three and one quarter years in which
the evaluation forms have been used
to provide a data base from which
such comparisons could be drawn.
According to Cramer, "the Univer
sity has shunted the evaluations onto
the Office of Academic Standing,"
which already has too much work to
do.
The survey process has grown
beyond its original function without
anyone overseeing its growth. No
comprehensive program for inform
ing students and faculty of the sur
vey's functions exists.
Faculty evaluation forms used by
the ASSU Course Guide — which ask
students to rate their professors' use
of course materials, accessibility out
side of class, interest in student opin
ions and other qualities — are also
used to make decisions on faculty
promotions and tenure.
But the decentralized way in which
the survey is conducted means that
no one is responsible for checking
that the survey measures what it is
supposed to or that people under
stand its proper uses.
Students often do not realize the
survey is used for purposes other
than the Course Guide. Many do not
read the statement of purpose in the
Course Guide, either. And more im
portantly, no notice of the survey's
purpose appears on the question
naire itself.
According to a political science
professor involved in survey research
here, not informing respondents of
the purpose of a questionnaire con
stitutes a violation of the "rules of the

game" of opinion research.
Chris Cramer, who heads this
year's ASSU course guide program,
agrees that both students and pro
fessors have misunderstood the na
ture of the questionnaires. He says
the a course guide intern is working
on a program to explain the survey's
purposes to freshmen beginning
next term.
"Most students don't realize
they're a quality control on a pro
fessor," he said.
The ASSU does not have primary
responsibility for the survey. The
University sends course evaluation
forms to professors, who then give
them to their students. The com
pleted forms are sent to the Office of
Academic Standing.
According to Jim Coleman of the
Office of Academic Standing, the
data is processed through a scanning
machine and the forms are sent back
to individual professors. One month
later, a report which forms the basis
of the ASSU Course Guide is printed.
Professors, whose participation is

voluntary but strongly encouraged
by the Faculty Senate, receive a copy
of this report. Departments can also
receive a copy.
Departments are free to use or ig
nore these reports as they wish. One
professor worries that some depart
ments will "appeal to the magic of
numbers" and use the hard statistics
the questionnaires provide without
considering if these numbers in fact
measure what they are supposed to
measure.
"When the system was set up, it
wasn't set up for the different func
tions" it is now used for, Coleman
said. "Any use beyond feedback was
not originally considered."
Hiring and firing decisions now
could be partially based on questions
that are vague, measure popularity
rather than skill, or do not take differ
ing circumstances into account.
For instance, Coleman said, the
system currently used does not ex
amine whether new professors score
lower than experienced professors,
whether required courses are less

popular than electives, whether
courses in the hard sciences have dif
ferent scores than those in the hu
manities, or if teachers who are un
der consideration for tenure receive
higher scores from sympathetic stu
dents who know student evaluations
are a factor in the decision.
Such rigorous evaluations are pos
sible, Coleman said, but "we're just
beginning to get the material to do
it."
His office now has data from the
three and one quarter years in which
the evaluation forms have been used
to provide a data base from which
such comparisons could be drawn.
According to Cramer, "the Univer
sity has shunted the evaluations onto
the Office of Academic Standing,"
which already has too much work to
do.
The survey process has grown
beyond its original function without
anyone overseeing its growth. No
comprehensive program for inform
ing students and faculty of the sur
vey's functions exists.
Faculty evaluation forms used by
the ASSU Course Guide — which ask
students to rate their professors' use
of course materials, accessibility out
side of class, interest in student opin
ions and other qualities — are also
used to make decisions on faculty
promotions and tenure.
But the decentralized way in which
the survey is conducted means that
no one is responsible for checking
that the survey measures what it is
supposed to or that people under
stand its proper uses.
Students often do not realize the
survey is used for purposes other
than the Course Guide. Many do not
read the statement of purpose in the
Course Guide, either. And more im
portantly, no notice of the survey's
purpose appears on the question
naire itself.
According to a political science
professor involved in survey research
here, not informing respondents of
the purpose of a questionnaire con
stitutes a violation of the "rules of the

game" of opinion research.
Chris Cramer, who heads this
year's ASSU course guide program,
agrees that both students and pro
fessors have misunderstood the na
ture of the questionnaires. He says
the a course guide intern is working
on a program to explain the survey's
purposes to freshmen beginning
next term.
"Most students don't realize
they're a quality control on a pro
fessor," he said.
The ASSU does not have primary
responsibility for the survey. The
University sends course evaluation
forms to professors, who then give
them to their students. The com
pleted forms are sent to the Office of
Academic Standing.
According to Jim Coleman of the
Office of Academic Standing, the
data is processed through a scanning
machine and the forms are sent back
to individual professors. One month
later, a report which forms the basis
of the ASSU Course Guide is printed.
Professors, whose participation is

voluntary but strongly encouraged
by the Faculty Senate, receive a copy
of this report. Departments can also
receive a copy.
Departments are free to use or ig
nore these reports as they wish. One
professor worries that some depart
ments will "appeal to the magic of
numbers" and use the hard statistics
the questionnaires provide without
considering if these numbers in fact
measure what they are supposed to
measure.
"When the system was set up, it
wasn't set up for the different func
tions" it is now used for, Coleman
said. "Any use beyond feedback was
not originally considered."
Hiring and firing decisions now
could be partially based on questions
that are vague, measure popularity
rather than skill, or do not take differ
ing circumstances into account.
For instance, Coleman said, the
system currently used does not ex
amine whether new professors score
lower than experienced professors,
whether required courses are less

popular than electives, whether
courses in the hard sciences have dif
ferent scores than those in the hu
manities, or if teachers who are un
der consideration for tenure receive
higher scores from sympathetic stu
dents who know student evaluations
are a factor in the decision.
Such rigorous evaluations are pos
sible, Coleman said, but "we're just
beginning to get the material to do
it."
His office now has data from the
three and one quarter years in which
the evaluation forms have been used
to provide a data base from which
such comparisons could be drawn.
According to Cramer, "the Univer
sity has shunted the evaluations onto
the Office of Academic Standing,"
which already has too much work to
do.
The survey process has grown
beyond its original function without
anyone overseeing its growth. No
comprehensive program for inform
ing students and faculty of the sur
vey's functions exists.
Faculty evaluation forms used by
the ASSU Course Guide — which ask
students to rate their professors' use
of course materials, accessibility out
side of class, interest in student opin
ions and other qualities — are also
used to make decisions on faculty
promotions and tenure.
But the decentralized way in which
the survey is conducted means that
no one is responsible for checking
that the survey measures what it is
supposed to or that people under
stand its proper uses.
Students often do not realize the
survey is used for purposes other
than the Course Guide. Many do not
read the statement of purpose in the
Course Guide, either. And more im
portantly, no notice of the survey's
purpose appears on the question
naire itself.
According to a political science
professor involved in survey research
here, not informing respondents of
the purpose of a questionnaire con
stitutes a violation of the "rules of the

game" of opinion research.
Chris Cramer, who heads this
year's ASSU course guide program,
agrees that both students and pro
fessors have misunderstood the na
ture of the questionnaires. He says
the a course guide intern is working
on a program to explain the survey's
purposes to freshmen beginning
next term.
"Most students don't realize
they're a quality control on a pro
fessor," he said.
The ASSU does not have primary
responsibility for the survey. The
University sends course evaluation
forms to professors, who then give
them to their students. The com
pleted forms are sent to the Office of
Academic Standing.
According to Jim Coleman of the
Office of Academic Standing, the
data is processed through a scanning
machine and the forms are sent back
to individual professors. One month
later, a report which forms the basis
of the ASSU Course Guide is printed.
Professors, whose participation is

voluntary but strongly encouraged
by the Faculty Senate, receive a copy
of this report. Departments can also
receive a copy.
Departments are free to use or ig
nore these reports as they wish. One
professor worries that some depart
ments will "appeal to the magic of
numbers" and use the hard statistics
the questionnaires provide without
considering if these numbers in fact
measure what they are supposed to
measure.
"When the system was set up, it
wasn't set up for the different func
tions" it is now used for, Coleman
said. "Any use beyond feedback was
not originally considered."
Hiring and firing decisions now
could be partially based on questions
that are vague, measure popularity
rather than skill, or do not take differ
ing circumstances into account.
For instance, Coleman said, the
system currently used does not ex
amine whether new professors score
lower than experienced professors,
whether required courses are less

popular than electives, whether
courses in the hard sciences have dif
ferent scores than those in the hu
manities, or if teachers who are un
der consideration for tenure receive
higher scores from sympathetic stu
dents who know student evaluations
are a factor in the decision.
Such rigorous evaluations are pos
sible, Coleman said, but "we're just
beginning to get the material to do
it."
His office now has data from the
three and one quarter years in which
the evaluation forms have been used
to provide a data base from which
such comparisons could be drawn.
According to Cramer, "the Univer
sity has shunted the evaluations onto
the Office of Academic Standing,"
which already has too much work to
do.
The survey process has grown
beyond its original function without
anyone overseeing its growth. No
comprehensive program for inform
ing students and faculty of the sur
vey's functions exists.
Faculty evaluation forms used by
the ASSU Course Guide — which ask
students to rate their professors' use
of course materials, accessibility out
side of class, interest in student opin
ions and other qualities — are also
used to make decisions on faculty
promotions and tenure.
But the decentralized way in which
the survey is conducted means that
no one is responsible for checking
that the survey measures what it is
supposed to or that people under
stand its proper uses.
Students often do not realize the
survey is used for purposes other
than the Course Guide. Many do not
read the statement of purpose in the
Course Guide, either. And more im
portantly, no notice of the survey's
purpose appears on the question
naire itself.
According to a political science
professor involved in survey research
here, not informing respondents of
the purpose of a questionnaire con
stitutes a violation of the "rules of the

game" of opinion research.
Chris Cramer, who heads this
year's ASSU course guide program,
agrees that both students and pro
fessors have misunderstood the na
ture of the questionnaires. He says
the a course guide intern is working
on a program to explain the survey's
purposes to freshmen beginning
next term.
"Most students don't realize
they're a quality control on a pro
fessor," he said.
The ASSU does not have primary
responsibility for the survey. The
University sends course evaluation
forms to professors, who then give
them to their students. The com
pleted forms are sent to the Office of
Academic Standing.
According to Jim Coleman of the
Office of Academic Standing, the
data is processed through a scanning
machine and the forms are sent back
to individual professors. One month
later, a report which forms the basis
of the ASSU Course Guide is printed.
Professors, whose participation is

voluntary but strongly encouraged
by the Faculty Senate, receive a copy
of this report. Departments can also
receive a copy.
Departments are free to use or ig
nore these reports as they wish. One
professor worries that some depart
ments will "appeal to the magic of
numbers" and use the hard statistics
the questionnaires provide without
considering if these numbers in fact
measure what they are supposed to
measure.
"When the system was set up, it
wasn't set up for the different func
tions" it is now used for, Coleman
said. "Any use beyond feedback was
not originally considered."
Hiring and firing decisions now
could be partially based on questions
that are vague, measure popularity
rather than skill, or do not take differ
ing circumstances into account.
For instance, Coleman said, the
system currently used does not ex
amine whether new professors score
lower than experienced professors,
whether required courses are less

popular than electives, whether
courses in the hard sciences have dif
ferent scores than those in the hu
manities, or if teachers who are un
der consideration for tenure receive
higher scores from sympathetic stu
dents who know student evaluations
are a factor in the decision.
Such rigorous evaluations are pos
sible, Coleman said, but "we're just
beginning to get the material to do
it."
His office now has data from the
three and one quarter years in which
the evaluation forms have been used
to provide a data base from which
such comparisons could be drawn.
According to Cramer, "the Univer
sity has shunted the evaluations onto
the Office of Academic Standing,"
which already has too much work to
do.
The survey process has grown
beyond its original function without
anyone overseeing its growth. No
comprehensive program for inform
ing students and faculty of the sur
vey's functions exists.
Faculty evaluation forms used by
the ASSU Course Guide — which ask
students to rate their professors' use
of course materials, accessibility out
side of class, interest in student opin
ions and other qualities — are also
used to make decisions on faculty
promotions and tenure.
But the decentralized way in which
the survey is conducted means that
no one is responsible for checking
that the survey measures what it is
supposed to or that people under
stand its proper uses.
Students often do not realize the
survey is used for purposes other
than the Course Guide. Many do not
read the statement of purpose in the
Course Guide, either. And more im
portantly, no notice of the survey's
purpose appears on the question
naire itself.
According to a political science
professor involved in survey research
here, not informing respondents of
the purpose of a questionnaire con
stitutes a violation of the "rules of the

game" of opinion research.
Chris Cramer, who heads this
year's ASSU course guide program,
agrees that both students and pro
fessors have misunderstood the na
ture of the questionnaires. He says
the a course guide intern is working
on a program to explain the survey's
purposes to freshmen beginning
next term.
"Most students don't realize
they're a quality control on a pro
fessor," he said.
The ASSU does not have primary
responsibility for the survey. The
University sends course evaluation
forms to professors, who then give
them to their students. The com
pleted forms are sent to the Office of
Academic Standing.
According to Jim Coleman of the
Office of Academic Standing, the
data is processed through a scanning
machine and the forms are sent back
to individual professors. One month
later, a report which forms the basis
of the ASSU Course Guide is printed.
Professors, whose participation is

voluntary but strongly encouraged
by the Faculty Senate, receive a copy
of this report. Departments can also
receive a copy.
Departments are free to use or ig
nore these reports as they wish. One
professor worries that some depart
ments will "appeal to the magic of
numbers" and use the hard statistics
the questionnaires provide without
considering if these numbers in fact
measure what they are supposed to
measure.
"When the system was set up, it
wasn't set up for the different func
tions" it is now used for, Coleman
said. "Any use beyond feedback was
not originally considered."
Hiring and firing decisions now
could be partially based on questions
that are vague, measure popularity
rather than skill, or do not take differ
ing circumstances into account.
For instance, Coleman said, the
system currently used does not ex
amine whether new professors score
lower than experienced professors,
whether required courses are less

popular than electives, whether
courses in the hard sciences have dif
ferent scores than those in the hu
manities, or if teachers who are un
der consideration for tenure receive
higher scores from sympathetic stu
dents who know student evaluations
are a factor in the decision.
Such rigorous evaluations are pos
sible, Coleman said, but "we're just
beginning to get the material to do
it."
His office now has data from the
three and one quarter years in which
the evaluation forms have been used
to provide a data base from which
such comparisons could be drawn.
According to Cramer, "the Univer
sity has shunted the evaluations onto
the Office of Academic Standing,"
which already has too much work to
do.
The survey process has grown
beyond its original function without
anyone overseeing its growth. No
comprehensive program for inform
ing students and faculty of the sur
vey's functions exists.
Faculty evaluation forms used by
the ASSU Course Guide — which ask
students to rate their professors' use
of course materials, accessibility out
side of class, interest in student opin
ions and other qualities — are also
used to make decisions on faculty
promotions and tenure.
But the decentralized way in which
the survey is conducted means that
no one is responsible for checking
that the survey measures what it is
supposed to or that people under
stand its proper uses.
Students often do not realize the
survey is used for purposes other
than the Course Guide. Many do not
read the statement of purpose in the
Course Guide, either. And more im
portantly, no notice of the survey's
purpose appears on the question
naire itself.
According to a political science
professor involved in survey research
here, not informing respondents of
the purpose of a questionnaire con
stitutes a violation of the "rules of the

game" of opinion research.
Chris Cramer, who heads this
year's ASSU course guide program,
agrees that both students and pro
fessors have misunderstood the na
ture of the questionnaires. He says
the a course guide intern is working
on a program to explain the survey's
purposes to freshmen beginning
next term.
"Most students don't realize
they're a quality control on a pro
fessor," he said.
The ASSU does not have primary
responsibility for the survey. The
University sends course evaluation
forms to professors, who then give
them to their students. The com
pleted forms are sent to the Office of
Academic Standing.
According to Jim Coleman of the
Office of Academic Standing, the
data is processed through a scanning
machine and the forms are sent back
to individual professors. One month
later, a report which forms the basis
of the ASSU Course Guide is printed.
Professors, whose participation is

voluntary but strongly encouraged
by the Faculty Senate, receive a copy
of this report. Departments can also
receive a copy.
Departments are free to use or ig
nore these reports as they wish. One
professor worries that some depart
ments will "appeal to the magic of
numbers" and use the hard statistics
the questionnaires provide without
considering if these numbers in fact
measure what they are supposed to
measure.
"When the system was set up, it
wasn't set up for the different func
tions" it is now used for, Coleman
said. "Any use beyond feedback was
not originally considered."
Hiring and firing decisions now
could be partially based on questions
that are vague, measure popularity
rather than skill, or do not take differ
ing circumstances into account.
For instance, Coleman said, the
system currently used does not ex
amine whether new professors score
lower than experienced professors,
whether required courses are less

popular than electives, whether
courses in the hard sciences have dif
ferent scores than those in the hu
manities, or if teachers who are un
der consideration for tenure receive
higher scores from sympathetic stu
dents who know student evaluations
are a factor in the decision.
Such rigorous evaluations are pos
sible, Coleman said, but "we're just
beginning to get the material to do
it."
His office now has data from the
three and one quarter years in which
the evaluation forms have been used
to provide a data base from which
such comparisons could be drawn.
According to Cramer, "the Univer
sity has shunted the evaluations onto
the Office of Academic Standing,"
which already has too much work to
do.
The survey process has grown
beyond its original function without
anyone overseeing its growth. No
comprehensive program for inform
ing students and faculty of the sur
vey's functions exists.
Faculty evaluation forms used by
the ASSU Course Guide — which ask
students to rate their professors' use
of course materials, accessibility out
side of class, interest in student opin
ions and other qualities — are also
used to make decisions on faculty
promotions and tenure.
But the decentralized way in which
the survey is conducted means that
no one is responsible for checking
that the survey measures what it is
supposed to or that people under
stand its proper uses.
Students often do not realize the
survey is used for purposes other
than the Course Guide. Many do not
read the statement of purpose in the
Course Guide, either. And more im
portantly, no notice of the survey's
purpose appears on the question
naire itself.
According to a political science
professor involved in survey research
here, not informing respondents of
the purpose of a questionnaire con
stitutes a violation of the "rules of the

game" of opinion research.
Chris Cramer, who heads this
year's ASSU course guide program,
agrees that both students and pro
fessors have misunderstood the na
ture of the questionnaires. He says
the a course guide intern is working
on a program to explain the survey's
purposes to freshmen beginning
next term.
"Most students don't realize
they're a quality control on a pro
fessor," he said.
The ASSU does not have primary
responsibility for the survey. The
University sends course evaluation
forms to professors, who then give
them to their students. The com
pleted forms are sent to the Office of
Academic Standing.
According to Jim Coleman of the
Office of Academic Standing, the
data is processed through a scanning
machine and the forms are sent back
to individual professors. One month
later, a report which forms the basis
of the ASSU Course Guide is printed.
Professors, whose participation is

voluntary but strongly encouraged
by the Faculty Senate, receive a copy
of this report. Departments can also
receive a copy.
Departments are free to use or ig
nore these reports as they wish. One
professor worries that some depart
ments will "appeal to the magic of
numbers" and use the hard statistics
the questionnaires provide without
considering if these numbers in fact
measure what they are supposed to
measure.
"When the system was set up, it
wasn't set up for the different func
tions" it is now used for, Coleman
said. "Any use beyond feedback was
not originally considered."
Hiring and firing decisions now
could be partially based on questions
that are vague, measure popularity
rather than skill, or do not take differ
ing circumstances into account.
For instance, Coleman said, the
system currently used does not ex
amine whether new professors score
lower than experienced professors,
whether required courses are less

popular than electives, whether
courses in the hard sciences have dif
ferent scores than those in the hu
manities, or if teachers who are un
der consideration for tenure receive
higher scores from sympathetic stu
dents who know student evaluations
are a factor in the decision.
Such rigorous evaluations are pos
sible, Coleman said, but "we're just
beginning to get the material to do
it."
His office now has data from the
three and one quarter years in which
the evaluation forms have been used
to provide a data base from which
such comparisons could be drawn.
According to Cramer, "the Univer
sity has shunted the evaluations onto
the Office of Academic Standing,"
which already has too much work to
do.
The survey process has grown
beyond its original function without
anyone overseeing its growth. No
comprehensive program for inform
ing students and faculty of the sur
vey's functions exists.
