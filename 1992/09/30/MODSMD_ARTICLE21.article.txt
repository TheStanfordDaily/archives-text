# Science
## Low-power circuits blaze path for future 
### Aaron Thode Editorial staff 
Electronic 'neuron' will be a crucial step in making neural nets, space systems

nanojoule of energy per instruction (PI), by the time
the signal is routed through the memory, controller
and bus, a typical computer workstation like the
Sun SPARCstation has spent about one microjoule
PI.
If one could shrink the size of the chips and stuff
them close together, the computer would consume
less power. But this technique cannot be used with
today's chips, said Williamson.
"You can cram a bunch of memory chips togeth
er to squeeze one gigabyte of memory into a few
cubic inches. The problem is, you've built yourself a
hair dryer. It will melt!"
So the key to reducing a computer's total power
consumption is to reduce the heat generated by an
individual chip. Stanford's solution has been to
lower the chip's supply voltage.
Traditionally all chips have sent and received sig
nals at either zero or five volts in order to insure
that the binary one is easily distinguished from
binary zero. "Historically, there has been a strong
bias against fooling with the supply voltage"
because all manufacturers adhere to the standard,
Burr said.
But the popularity of small laptop computers
encouraged semiconductor manufacturers to test
chips with three or even two volts of applied voltage.
Stanford has gone much further, building chips that
require less than .3 volts. Doing this has reduced the
chip's power consumption by about a thousand
times.
There is a cost to consuming less power. The new
chips run slightly slower than conventional chips.
Lowering the applied voltage also increases the
chance that random noise will ruin a signal, so mis
takes occur more frequently. However, since paral
lel networks use many processors, they are highly
redundant, so the lower performance of an individ
ual processor has a much smaller impact on the per
formance of the entire computer.
Stanford's Center for Integrated Systems "has
built a number of building blocks for computation
al structures," such as adders, multipliers and
memory cells. "We know they work," Burr said,
"but we can't tell how well they work."
To fix that problem Stanford is collaborating
with Intel Corp. to fabricate circuits that are self
testing. If an error occurs while running, it will stop
and flag the monitoring instruments.
By lowering the supply voltage and exploiting
other techniques, Burr hopes to reach a point next
year where the new chips will consume just 30 fem
tojoules PI, which is almost as efficient as a biologi
cal neuron. To lower the power consumption fur
ther, the chips would have to be cooled to reduce
thermal noise signals.
Both Burr and Williamson hope that these low
power chips can be assembled into complete com
puter processors on a silicon platform less than a
square inch in area.
These processors could then be crammed into
very dense cubes a cubic-inch in size. The close prox
imity of its components will vastly improve the com
puter's overall energy efficiency, and pave the way
for large-scale neural networks.
Stanford's work is currently being funded by
NASA and CIS at about $100,000 per year. Howev
er, NASA's three-year contract will end by Decem
ber, and Burr is uncertain of the program's future;
he said he is trying to combine the group's work
with the development of a Mars mini-rover at the
Jet Propulsion Laboratory.

systems require, because the sooner the processor
finishes each task, the faster the entire program will
run. As a result, modern supercomputers, like
Grays, expend large amounts of energy to push
sophisticated processors to their fastest limits.
Neural networks and the human brain take the
opposite approach: They tackle several instructions
simultaneously using many simple processors.
Because so many processors are involved, the fail
ure of a few will not disrupt the computer. Thus
parallel computers are more flexible than, and — for
many signal processing applications — faster than,
conventional computers.
Most past computer research has focused on
"maximizing the performance of a single processor
rather than minimizing the energy required by
many processors," Burr said.
But as interest in neural networks grew, so did
the need for an energy-efficient processor. Also,
Prof. Roger Williamson of Stanford's Space
Telecommunications and Radioscience Laboratory
wished to reduce the amount of power space experi
ments consumed, so that they could be smaller and
launched faster. All these factors encouraged Burr to
begin working on high-efficiency processors in 1988.
There are several reasons why computers are rel
atively energy inefficient, Burr said. The first is that
computers use a great deal of energy transferring
signals between processors.
Even though a single chip consumes just one

If your brain were made of computer chips, it
would explode.
To laugh, love, think, imagine and create — tens
of billions of neurons accomplish these actions while
consuming just a few watts of power. If our neurons
were replaced with today's state-of-the-art proces
sors, our heads would require megawatts of power,
most of which would be re-radiated off as heat, boil
ing our brains.
Prof. James Burr and graduate students Sabeer
Bhatia and Boyd Fowler have constructed transis
tors and chips that use a thousand times less power
than conventional circuits to accomplish basic tasks
like addition and memory storage. If they reach
their goals, by next year they will have built simple
processors that consume only 30 times as much
power as a brain neuron.
Such chips would be a major breakthrough
towards creating "neural networks," which many
computer scientists consider the most promising
field of supercomputing.
Most computer systems over the past fifty years
have been built with a "sequential" philosophy. In
other words, given a set of instructions, a computer
completes each task before continuing with the
next.
Speed, not energy-efficiency, is what sequential


Joshua Folk — Daily
Prof. James Burr carefully positions a silicon wafer containing low-power test circuits.The circuits use
a thousand times less power than conventional circuits to accomplish basic tasks like addition and
memory storage. Burr hopes these simple devices will eventually lead to much smaller and faster com
puters.
Electronic 'neuron' will be a crucial step in making neural nets, space systems

nanojoule of energy per instruction (PI), by the time
the signal is routed through the memory, controller
and bus, a typical computer workstation like the
Sun SPARCstation has spent about one microjoule
PI.
If one could shrink the size of the chips and stuff
them close together, the computer would consume
less power. But this technique cannot be used with
today's chips, said Williamson.
"You can cram a bunch of memory chips togeth
er to squeeze one gigabyte of memory into a few
cubic inches. The problem is, you've built yourself a
hair dryer. It will melt!"
So the key to reducing a computer's total power
consumption is to reduce the heat generated by an
individual chip. Stanford's solution has been to
lower the chip's supply voltage.
Traditionally all chips have sent and received sig
nals at either zero or five volts in order to insure
that the binary one is easily distinguished from
binary zero. "Historically, there has been a strong
bias against fooling with the supply voltage"
because all manufacturers adhere to the standard,
Burr said.
But the popularity of small laptop computers
encouraged semiconductor manufacturers to test
chips with three or even two volts of applied voltage.
Stanford has gone much further, building chips that
require less than .3 volts. Doing this has reduced the
chip's power consumption by about a thousand
times.
There is a cost to consuming less power. The new
chips run slightly slower than conventional chips.
Lowering the applied voltage also increases the
chance that random noise will ruin a signal, so mis
takes occur more frequently. However, since paral
lel networks use many processors, they are highly
redundant, so the lower performance of an individ
ual processor has a much smaller impact on the per
formance of the entire computer.
Stanford's Center for Integrated Systems "has
built a number of building blocks for computation
al structures," such as adders, multipliers and
memory cells. "We know they work," Burr said,
"but we can't tell how well they work."
To fix that problem Stanford is collaborating
with Intel Corp. to fabricate circuits that are self
testing. If an error occurs while running, it will stop
and flag the monitoring instruments.
By lowering the supply voltage and exploiting
other techniques, Burr hopes to reach a point next
year where the new chips will consume just 30 fem
tojoules PI, which is almost as efficient as a biologi
cal neuron. To lower the power consumption fur
ther, the chips would have to be cooled to reduce
thermal noise signals.
Both Burr and Williamson hope that these low
power chips can be assembled into complete com
puter processors on a silicon platform less than a
square inch in area.
These processors could then be crammed into
very dense cubes a cubic-inch in size. The close prox
imity of its components will vastly improve the com
puter's overall energy efficiency, and pave the way
for large-scale neural networks.
Stanford's work is currently being funded by
NASA and CIS at about $100,000 per year. Howev
er, NASA's three-year contract will end by Decem
ber, and Burr is uncertain of the program's future;
he said he is trying to combine the group's work
with the development of a Mars mini-rover at the
Jet Propulsion Laboratory.

systems require, because the sooner the processor
finishes each task, the faster the entire program will
run. As a result, modern supercomputers, like
Grays, expend large amounts of energy to push
sophisticated processors to their fastest limits.
Neural networks and the human brain take the
opposite approach: They tackle several instructions
simultaneously using many simple processors.
Because so many processors are involved, the fail
ure of a few will not disrupt the computer. Thus
parallel computers are more flexible than, and — for
many signal processing applications — faster than,
conventional computers.
Most past computer research has focused on
"maximizing the performance of a single processor
rather than minimizing the energy required by
many processors," Burr said.
But as interest in neural networks grew, so did
the need for an energy-efficient processor. Also,
Prof. Roger Williamson of Stanford's Space
Telecommunications and Radioscience Laboratory
wished to reduce the amount of power space experi
ments consumed, so that they could be smaller and
launched faster. All these factors encouraged Burr to
begin working on high-efficiency processors in 1988.
There are several reasons why computers are rel
atively energy inefficient, Burr said. The first is that
computers use a great deal of energy transferring
signals between processors.
Even though a single chip consumes just one

If your brain were made of computer chips, it
would explode.
To laugh, love, think, imagine and create — tens
of billions of neurons accomplish these actions while
consuming just a few watts of power. If our neurons
were replaced with today's state-of-the-art proces
sors, our heads would require megawatts of power,
most of which would be re-radiated off as heat, boil
ing our brains.
Prof. James Burr and graduate students Sabeer
Bhatia and Boyd Fowler have constructed transis
tors and chips that use a thousand times less power
than conventional circuits to accomplish basic tasks
like addition and memory storage. If they reach
their goals, by next year they will have built simple
processors that consume only 30 times as much
power as a brain neuron.
Such chips would be a major breakthrough
towards creating "neural networks," which many
computer scientists consider the most promising
field of supercomputing.
Most computer systems over the past fifty years
have been built with a "sequential" philosophy. In
other words, given a set of instructions, a computer
completes each task before continuing with the
next.
Speed, not energy-efficiency, is what sequential


Joshua Folk — Daily
Prof. James Burr carefully positions a silicon wafer containing low-power test circuits.The circuits use
a thousand times less power than conventional circuits to accomplish basic tasks like addition and
memory storage. Burr hopes these simple devices will eventually lead to much smaller and faster com
puters.
Electronic 'neuron' will be a crucial step in making neural nets, space systems

nanojoule of energy per instruction (PI), by the time
the signal is routed through the memory, controller
and bus, a typical computer workstation like the
Sun SPARCstation has spent about one microjoule
PI.
If one could shrink the size of the chips and stuff
them close together, the computer would consume
less power. But this technique cannot be used with
today's chips, said Williamson.
"You can cram a bunch of memory chips togeth
er to squeeze one gigabyte of memory into a few
cubic inches. The problem is, you've built yourself a
hair dryer. It will melt!"
So the key to reducing a computer's total power
consumption is to reduce the heat generated by an
individual chip. Stanford's solution has been to
lower the chip's supply voltage.
Traditionally all chips have sent and received sig
nals at either zero or five volts in order to insure
that the binary one is easily distinguished from
binary zero. "Historically, there has been a strong
bias against fooling with the supply voltage"
because all manufacturers adhere to the standard,
Burr said.
But the popularity of small laptop computers
encouraged semiconductor manufacturers to test
chips with three or even two volts of applied voltage.
Stanford has gone much further, building chips that
require less than .3 volts. Doing this has reduced the
chip's power consumption by about a thousand
times.
There is a cost to consuming less power. The new
chips run slightly slower than conventional chips.
Lowering the applied voltage also increases the
chance that random noise will ruin a signal, so mis
takes occur more frequently. However, since paral
lel networks use many processors, they are highly
redundant, so the lower performance of an individ
ual processor has a much smaller impact on the per
formance of the entire computer.
Stanford's Center for Integrated Systems "has
built a number of building blocks for computation
al structures," such as adders, multipliers and
memory cells. "We know they work," Burr said,
"but we can't tell how well they work."
To fix that problem Stanford is collaborating
with Intel Corp. to fabricate circuits that are self
testing. If an error occurs while running, it will stop
and flag the monitoring instruments.
By lowering the supply voltage and exploiting
other techniques, Burr hopes to reach a point next
year where the new chips will consume just 30 fem
tojoules PI, which is almost as efficient as a biologi
cal neuron. To lower the power consumption fur
ther, the chips would have to be cooled to reduce
thermal noise signals.
Both Burr and Williamson hope that these low
power chips can be assembled into complete com
puter processors on a silicon platform less than a
square inch in area.
These processors could then be crammed into
very dense cubes a cubic-inch in size. The close prox
imity of its components will vastly improve the com
puter's overall energy efficiency, and pave the way
for large-scale neural networks.
Stanford's work is currently being funded by
NASA and CIS at about $100,000 per year. Howev
er, NASA's three-year contract will end by Decem
ber, and Burr is uncertain of the program's future;
he said he is trying to combine the group's work
with the development of a Mars mini-rover at the
Jet Propulsion Laboratory.

systems require, because the sooner the processor
finishes each task, the faster the entire program will
run. As a result, modern supercomputers, like
Grays, expend large amounts of energy to push
sophisticated processors to their fastest limits.
Neural networks and the human brain take the
opposite approach: They tackle several instructions
simultaneously using many simple processors.
Because so many processors are involved, the fail
ure of a few will not disrupt the computer. Thus
parallel computers are more flexible than, and — for
many signal processing applications — faster than,
conventional computers.
Most past computer research has focused on
"maximizing the performance of a single processor
rather than minimizing the energy required by
many processors," Burr said.
But as interest in neural networks grew, so did
the need for an energy-efficient processor. Also,
Prof. Roger Williamson of Stanford's Space
Telecommunications and Radioscience Laboratory
wished to reduce the amount of power space experi
ments consumed, so that they could be smaller and
launched faster. All these factors encouraged Burr to
begin working on high-efficiency processors in 1988.
There are several reasons why computers are rel
atively energy inefficient, Burr said. The first is that
computers use a great deal of energy transferring
signals between processors.
Even though a single chip consumes just one

If your brain were made of computer chips, it
would explode.
To laugh, love, think, imagine and create — tens
of billions of neurons accomplish these actions while
consuming just a few watts of power. If our neurons
were replaced with today's state-of-the-art proces
sors, our heads would require megawatts of power,
most of which would be re-radiated off as heat, boil
ing our brains.
Prof. James Burr and graduate students Sabeer
Bhatia and Boyd Fowler have constructed transis
tors and chips that use a thousand times less power
than conventional circuits to accomplish basic tasks
like addition and memory storage. If they reach
their goals, by next year they will have built simple
processors that consume only 30 times as much
power as a brain neuron.
Such chips would be a major breakthrough
towards creating "neural networks," which many
computer scientists consider the most promising
field of supercomputing.
Most computer systems over the past fifty years
have been built with a "sequential" philosophy. In
other words, given a set of instructions, a computer
completes each task before continuing with the
next.
Speed, not energy-efficiency, is what sequential


Joshua Folk — Daily
Prof. James Burr carefully positions a silicon wafer containing low-power test circuits.The circuits use
a thousand times less power than conventional circuits to accomplish basic tasks like addition and
memory storage. Burr hopes these simple devices will eventually lead to much smaller and faster com
puters.
Electronic 'neuron' will be a crucial step in making neural nets, space systems

nanojoule of energy per instruction (PI), by the time
the signal is routed through the memory, controller
and bus, a typical computer workstation like the
Sun SPARCstation has spent about one microjoule
PI.
If one could shrink the size of the chips and stuff
them close together, the computer would consume
less power. But this technique cannot be used with
today's chips, said Williamson.
"You can cram a bunch of memory chips togeth
er to squeeze one gigabyte of memory into a few
cubic inches. The problem is, you've built yourself a
hair dryer. It will melt!"
So the key to reducing a computer's total power
consumption is to reduce the heat generated by an
individual chip. Stanford's solution has been to
lower the chip's supply voltage.
Traditionally all chips have sent and received sig
nals at either zero or five volts in order to insure
that the binary one is easily distinguished from
binary zero. "Historically, there has been a strong
bias against fooling with the supply voltage"
because all manufacturers adhere to the standard,
Burr said.
But the popularity of small laptop computers
encouraged semiconductor manufacturers to test
chips with three or even two volts of applied voltage.
Stanford has gone much further, building chips that
require less than .3 volts. Doing this has reduced the
chip's power consumption by about a thousand
times.
There is a cost to consuming less power. The new
chips run slightly slower than conventional chips.
Lowering the applied voltage also increases the
chance that random noise will ruin a signal, so mis
takes occur more frequently. However, since paral
lel networks use many processors, they are highly
redundant, so the lower performance of an individ
ual processor has a much smaller impact on the per
formance of the entire computer.
Stanford's Center for Integrated Systems "has
built a number of building blocks for computation
al structures," such as adders, multipliers and
memory cells. "We know they work," Burr said,
"but we can't tell how well they work."
To fix that problem Stanford is collaborating
with Intel Corp. to fabricate circuits that are self
testing. If an error occurs while running, it will stop
and flag the monitoring instruments.
By lowering the supply voltage and exploiting
other techniques, Burr hopes to reach a point next
year where the new chips will consume just 30 fem
tojoules PI, which is almost as efficient as a biologi
cal neuron. To lower the power consumption fur
ther, the chips would have to be cooled to reduce
thermal noise signals.
Both Burr and Williamson hope that these low
power chips can be assembled into complete com
puter processors on a silicon platform less than a
square inch in area.
These processors could then be crammed into
very dense cubes a cubic-inch in size. The close prox
imity of its components will vastly improve the com
puter's overall energy efficiency, and pave the way
for large-scale neural networks.
Stanford's work is currently being funded by
NASA and CIS at about $100,000 per year. Howev
er, NASA's three-year contract will end by Decem
ber, and Burr is uncertain of the program's future;
he said he is trying to combine the group's work
with the development of a Mars mini-rover at the
Jet Propulsion Laboratory.

systems require, because the sooner the processor
finishes each task, the faster the entire program will
run. As a result, modern supercomputers, like
Grays, expend large amounts of energy to push
sophisticated processors to their fastest limits.
Neural networks and the human brain take the
opposite approach: They tackle several instructions
simultaneously using many simple processors.
Because so many processors are involved, the fail
ure of a few will not disrupt the computer. Thus
parallel computers are more flexible than, and — for
many signal processing applications — faster than,
conventional computers.
Most past computer research has focused on
"maximizing the performance of a single processor
rather than minimizing the energy required by
many processors," Burr said.
But as interest in neural networks grew, so did
the need for an energy-efficient processor. Also,
Prof. Roger Williamson of Stanford's Space
Telecommunications and Radioscience Laboratory
wished to reduce the amount of power space experi
ments consumed, so that they could be smaller and
launched faster. All these factors encouraged Burr to
begin working on high-efficiency processors in 1988.
There are several reasons why computers are rel
atively energy inefficient, Burr said. The first is that
computers use a great deal of energy transferring
signals between processors.
Even though a single chip consumes just one

If your brain were made of computer chips, it
would explode.
To laugh, love, think, imagine and create — tens
of billions of neurons accomplish these actions while
consuming just a few watts of power. If our neurons
were replaced with today's state-of-the-art proces
sors, our heads would require megawatts of power,
most of which would be re-radiated off as heat, boil
ing our brains.
Prof. James Burr and graduate students Sabeer
Bhatia and Boyd Fowler have constructed transis
tors and chips that use a thousand times less power
than conventional circuits to accomplish basic tasks
like addition and memory storage. If they reach
their goals, by next year they will have built simple
processors that consume only 30 times as much
power as a brain neuron.
Such chips would be a major breakthrough
towards creating "neural networks," which many
computer scientists consider the most promising
field of supercomputing.
Most computer systems over the past fifty years
have been built with a "sequential" philosophy. In
other words, given a set of instructions, a computer
completes each task before continuing with the
next.
Speed, not energy-efficiency, is what sequential


Joshua Folk — Daily
Prof. James Burr carefully positions a silicon wafer containing low-power test circuits.The circuits use
a thousand times less power than conventional circuits to accomplish basic tasks like addition and
memory storage. Burr hopes these simple devices will eventually lead to much smaller and faster com
puters.
Electronic 'neuron' will be a crucial step in making neural nets, space systems

nanojoule of energy per instruction (PI), by the time
the signal is routed through the memory, controller
and bus, a typical computer workstation like the
Sun SPARCstation has spent about one microjoule
PI.
If one could shrink the size of the chips and stuff
them close together, the computer would consume
less power. But this technique cannot be used with
today's chips, said Williamson.
"You can cram a bunch of memory chips togeth
er to squeeze one gigabyte of memory into a few
cubic inches. The problem is, you've built yourself a
hair dryer. It will melt!"
So the key to reducing a computer's total power
consumption is to reduce the heat generated by an
individual chip. Stanford's solution has been to
lower the chip's supply voltage.
Traditionally all chips have sent and received sig
nals at either zero or five volts in order to insure
that the binary one is easily distinguished from
binary zero. "Historically, there has been a strong
bias against fooling with the supply voltage"
because all manufacturers adhere to the standard,
Burr said.
But the popularity of small laptop computers
encouraged semiconductor manufacturers to test
chips with three or even two volts of applied voltage.
Stanford has gone much further, building chips that
require less than .3 volts. Doing this has reduced the
chip's power consumption by about a thousand
times.
There is a cost to consuming less power. The new
chips run slightly slower than conventional chips.
Lowering the applied voltage also increases the
chance that random noise will ruin a signal, so mis
takes occur more frequently. However, since paral
lel networks use many processors, they are highly
redundant, so the lower performance of an individ
ual processor has a much smaller impact on the per
formance of the entire computer.
Stanford's Center for Integrated Systems "has
built a number of building blocks for computation
al structures," such as adders, multipliers and
memory cells. "We know they work," Burr said,
"but we can't tell how well they work."
To fix that problem Stanford is collaborating
with Intel Corp. to fabricate circuits that are self
testing. If an error occurs while running, it will stop
and flag the monitoring instruments.
By lowering the supply voltage and exploiting
other techniques, Burr hopes to reach a point next
year where the new chips will consume just 30 fem
tojoules PI, which is almost as efficient as a biologi
cal neuron. To lower the power consumption fur
ther, the chips would have to be cooled to reduce
thermal noise signals.
Both Burr and Williamson hope that these low
power chips can be assembled into complete com
puter processors on a silicon platform less than a
square inch in area.
These processors could then be crammed into
very dense cubes a cubic-inch in size. The close prox
imity of its components will vastly improve the com
puter's overall energy efficiency, and pave the way
for large-scale neural networks.
Stanford's work is currently being funded by
NASA and CIS at about $100,000 per year. Howev
er, NASA's three-year contract will end by Decem
ber, and Burr is uncertain of the program's future;
he said he is trying to combine the group's work
with the development of a Mars mini-rover at the
Jet Propulsion Laboratory.

systems require, because the sooner the processor
finishes each task, the faster the entire program will
run. As a result, modern supercomputers, like
Grays, expend large amounts of energy to push
sophisticated processors to their fastest limits.
Neural networks and the human brain take the
opposite approach: They tackle several instructions
simultaneously using many simple processors.
Because so many processors are involved, the fail
ure of a few will not disrupt the computer. Thus
parallel computers are more flexible than, and — for
many signal processing applications — faster than,
conventional computers.
Most past computer research has focused on
"maximizing the performance of a single processor
rather than minimizing the energy required by
many processors," Burr said.
But as interest in neural networks grew, so did
the need for an energy-efficient processor. Also,
Prof. Roger Williamson of Stanford's Space
Telecommunications and Radioscience Laboratory
wished to reduce the amount of power space experi
ments consumed, so that they could be smaller and
launched faster. All these factors encouraged Burr to
begin working on high-efficiency processors in 1988.
There are several reasons why computers are rel
atively energy inefficient, Burr said. The first is that
computers use a great deal of energy transferring
signals between processors.
Even though a single chip consumes just one

If your brain were made of computer chips, it
would explode.
To laugh, love, think, imagine and create — tens
of billions of neurons accomplish these actions while
consuming just a few watts of power. If our neurons
were replaced with today's state-of-the-art proces
sors, our heads would require megawatts of power,
most of which would be re-radiated off as heat, boil
ing our brains.
Prof. James Burr and graduate students Sabeer
Bhatia and Boyd Fowler have constructed transis
tors and chips that use a thousand times less power
than conventional circuits to accomplish basic tasks
like addition and memory storage. If they reach
their goals, by next year they will have built simple
processors that consume only 30 times as much
power as a brain neuron.
Such chips would be a major breakthrough
towards creating "neural networks," which many
computer scientists consider the most promising
field of supercomputing.
Most computer systems over the past fifty years
have been built with a "sequential" philosophy. In
other words, given a set of instructions, a computer
completes each task before continuing with the
next.
Speed, not energy-efficiency, is what sequential


Joshua Folk — Daily
Prof. James Burr carefully positions a silicon wafer containing low-power test circuits.The circuits use
a thousand times less power than conventional circuits to accomplish basic tasks like addition and
memory storage. Burr hopes these simple devices will eventually lead to much smaller and faster com
puters.
Electronic 'neuron' will be a crucial step in making neural nets, space systems

nanojoule of energy per instruction (PI), by the time
the signal is routed through the memory, controller
and bus, a typical computer workstation like the
Sun SPARCstation has spent about one microjoule
PI.
If one could shrink the size of the chips and stuff
them close together, the computer would consume
less power. But this technique cannot be used with
today's chips, said Williamson.
"You can cram a bunch of memory chips togeth
er to squeeze one gigabyte of memory into a few
cubic inches. The problem is, you've built yourself a
hair dryer. It will melt!"
So the key to reducing a computer's total power
consumption is to reduce the heat generated by an
individual chip. Stanford's solution has been to
lower the chip's supply voltage.
Traditionally all chips have sent and received sig
nals at either zero or five volts in order to insure
that the binary one is easily distinguished from
binary zero. "Historically, there has been a strong
bias against fooling with the supply voltage"
because all manufacturers adhere to the standard,
Burr said.
But the popularity of small laptop computers
encouraged semiconductor manufacturers to test
chips with three or even two volts of applied voltage.
Stanford has gone much further, building chips that
require less than .3 volts. Doing this has reduced the
chip's power consumption by about a thousand
times.
There is a cost to consuming less power. The new
chips run slightly slower than conventional chips.
Lowering the applied voltage also increases the
chance that random noise will ruin a signal, so mis
takes occur more frequently. However, since paral
lel networks use many processors, they are highly
redundant, so the lower performance of an individ
ual processor has a much smaller impact on the per
formance of the entire computer.
Stanford's Center for Integrated Systems "has
built a number of building blocks for computation
al structures," such as adders, multipliers and
memory cells. "We know they work," Burr said,
"but we can't tell how well they work."
To fix that problem Stanford is collaborating
with Intel Corp. to fabricate circuits that are self
testing. If an error occurs while running, it will stop
and flag the monitoring instruments.
By lowering the supply voltage and exploiting
other techniques, Burr hopes to reach a point next
year where the new chips will consume just 30 fem
tojoules PI, which is almost as efficient as a biologi
cal neuron. To lower the power consumption fur
ther, the chips would have to be cooled to reduce
thermal noise signals.
Both Burr and Williamson hope that these low
power chips can be assembled into complete com
puter processors on a silicon platform less than a
square inch in area.
These processors could then be crammed into
very dense cubes a cubic-inch in size. The close prox
imity of its components will vastly improve the com
puter's overall energy efficiency, and pave the way
for large-scale neural networks.
Stanford's work is currently being funded by
NASA and CIS at about $100,000 per year. Howev
er, NASA's three-year contract will end by Decem
ber, and Burr is uncertain of the program's future;
he said he is trying to combine the group's work
with the development of a Mars mini-rover at the
Jet Propulsion Laboratory.

systems require, because the sooner the processor
finishes each task, the faster the entire program will
run. As a result, modern supercomputers, like
Grays, expend large amounts of energy to push
sophisticated processors to their fastest limits.
Neural networks and the human brain take the
opposite approach: They tackle several instructions
simultaneously using many simple processors.
Because so many processors are involved, the fail
ure of a few will not disrupt the computer. Thus
parallel computers are more flexible than, and — for
many signal processing applications — faster than,
conventional computers.
Most past computer research has focused on
"maximizing the performance of a single processor
rather than minimizing the energy required by
many processors," Burr said.
But as interest in neural networks grew, so did
the need for an energy-efficient processor. Also,
Prof. Roger Williamson of Stanford's Space
Telecommunications and Radioscience Laboratory
wished to reduce the amount of power space experi
ments consumed, so that they could be smaller and
launched faster. All these factors encouraged Burr to
begin working on high-efficiency processors in 1988.
There are several reasons why computers are rel
atively energy inefficient, Burr said. The first is that
computers use a great deal of energy transferring
signals between processors.
Even though a single chip consumes just one

If your brain were made of computer chips, it
would explode.
To laugh, love, think, imagine and create — tens
of billions of neurons accomplish these actions while
consuming just a few watts of power. If our neurons
were replaced with today's state-of-the-art proces
sors, our heads would require megawatts of power,
most of which would be re-radiated off as heat, boil
ing our brains.
Prof. James Burr and graduate students Sabeer
Bhatia and Boyd Fowler have constructed transis
tors and chips that use a thousand times less power
than conventional circuits to accomplish basic tasks
like addition and memory storage. If they reach
their goals, by next year they will have built simple
processors that consume only 30 times as much
power as a brain neuron.
Such chips would be a major breakthrough
towards creating "neural networks," which many
computer scientists consider the most promising
field of supercomputing.
Most computer systems over the past fifty years
have been built with a "sequential" philosophy. In
other words, given a set of instructions, a computer
completes each task before continuing with the
next.
Speed, not energy-efficiency, is what sequential


Joshua Folk — Daily
Prof. James Burr carefully positions a silicon wafer containing low-power test circuits.The circuits use
a thousand times less power than conventional circuits to accomplish basic tasks like addition and
memory storage. Burr hopes these simple devices will eventually lead to much smaller and faster com
puters.
Electronic 'neuron' will be a crucial step in making neural nets, space systems

nanojoule of energy per instruction (PI), by the time
the signal is routed through the memory, controller
and bus, a typical computer workstation like the
Sun SPARCstation has spent about one microjoule
PI.
If one could shrink the size of the chips and stuff
them close together, the computer would consume
less power. But this technique cannot be used with
today's chips, said Williamson.
"You can cram a bunch of memory chips togeth
er to squeeze one gigabyte of memory into a few
cubic inches. The problem is, you've built yourself a
hair dryer. It will melt!"
So the key to reducing a computer's total power
consumption is to reduce the heat generated by an
individual chip. Stanford's solution has been to
lower the chip's supply voltage.
Traditionally all chips have sent and received sig
nals at either zero or five volts in order to insure
that the binary one is easily distinguished from
binary zero. "Historically, there has been a strong
bias against fooling with the supply voltage"
because all manufacturers adhere to the standard,
Burr said.
But the popularity of small laptop computers
encouraged semiconductor manufacturers to test
chips with three or even two volts of applied voltage.
Stanford has gone much further, building chips that
require less than .3 volts. Doing this has reduced the
chip's power consumption by about a thousand
times.
There is a cost to consuming less power. The new
chips run slightly slower than conventional chips.
Lowering the applied voltage also increases the
chance that random noise will ruin a signal, so mis
takes occur more frequently. However, since paral
lel networks use many processors, they are highly
redundant, so the lower performance of an individ
ual processor has a much smaller impact on the per
formance of the entire computer.
Stanford's Center for Integrated Systems "has
built a number of building blocks for computation
al structures," such as adders, multipliers and
memory cells. "We know they work," Burr said,
"but we can't tell how well they work."
To fix that problem Stanford is collaborating
with Intel Corp. to fabricate circuits that are self
testing. If an error occurs while running, it will stop
and flag the monitoring instruments.
By lowering the supply voltage and exploiting
other techniques, Burr hopes to reach a point next
year where the new chips will consume just 30 fem
tojoules PI, which is almost as efficient as a biologi
cal neuron. To lower the power consumption fur
ther, the chips would have to be cooled to reduce
thermal noise signals.
Both Burr and Williamson hope that these low
power chips can be assembled into complete com
puter processors on a silicon platform less than a
square inch in area.
These processors could then be crammed into
very dense cubes a cubic-inch in size. The close prox
imity of its components will vastly improve the com
puter's overall energy efficiency, and pave the way
for large-scale neural networks.
Stanford's work is currently being funded by
NASA and CIS at about $100,000 per year. Howev
er, NASA's three-year contract will end by Decem
ber, and Burr is uncertain of the program's future;
he said he is trying to combine the group's work
with the development of a Mars mini-rover at the
Jet Propulsion Laboratory.

systems require, because the sooner the processor
finishes each task, the faster the entire program will
run. As a result, modern supercomputers, like
Grays, expend large amounts of energy to push
sophisticated processors to their fastest limits.
Neural networks and the human brain take the
opposite approach: They tackle several instructions
simultaneously using many simple processors.
Because so many processors are involved, the fail
ure of a few will not disrupt the computer. Thus
parallel computers are more flexible than, and — for
many signal processing applications — faster than,
conventional computers.
Most past computer research has focused on
"maximizing the performance of a single processor
rather than minimizing the energy required by
many processors," Burr said.
But as interest in neural networks grew, so did
the need for an energy-efficient processor. Also,
Prof. Roger Williamson of Stanford's Space
Telecommunications and Radioscience Laboratory
wished to reduce the amount of power space experi
ments consumed, so that they could be smaller and
launched faster. All these factors encouraged Burr to
begin working on high-efficiency processors in 1988.
There are several reasons why computers are rel
atively energy inefficient, Burr said. The first is that
computers use a great deal of energy transferring
signals between processors.
Even though a single chip consumes just one

If your brain were made of computer chips, it
would explode.
To laugh, love, think, imagine and create — tens
of billions of neurons accomplish these actions while
consuming just a few watts of power. If our neurons
were replaced with today's state-of-the-art proces
sors, our heads would require megawatts of power,
most of which would be re-radiated off as heat, boil
ing our brains.
Prof. James Burr and graduate students Sabeer
Bhatia and Boyd Fowler have constructed transis
tors and chips that use a thousand times less power
than conventional circuits to accomplish basic tasks
like addition and memory storage. If they reach
their goals, by next year they will have built simple
processors that consume only 30 times as much
power as a brain neuron.
Such chips would be a major breakthrough
towards creating "neural networks," which many
computer scientists consider the most promising
field of supercomputing.
Most computer systems over the past fifty years
have been built with a "sequential" philosophy. In
other words, given a set of instructions, a computer
completes each task before continuing with the
next.
Speed, not energy-efficiency, is what sequential


Joshua Folk — Daily
Prof. James Burr carefully positions a silicon wafer containing low-power test circuits.The circuits use
a thousand times less power than conventional circuits to accomplish basic tasks like addition and
memory storage. Burr hopes these simple devices will eventually lead to much smaller and faster com
puters.
