# Science
## The 						BIG One is more likely than ever 
### Katharine Miller 
•Ten years after the Loma Prieta
earthquake caused enormous dam
age in the Bay Area in 1989, the
results of studying that quake are in:
things are worse than we thought.
Last week, the U.S. Geological
Survey revised its predictions for
the probability that the Big One will
hit somewhere in the San Francisco
Bay region. In 1990, they said there
was a 67-percent chance of a quake
greater than 7.0 M (moments of
magnitude) hitting the region by
2020.
Now, 10 years later, the predic
tion is a 70-percent chance of a 6.7-
M quake or greater before the year
2030, and an 80-percent chance of a
quake 6.0- to 6.6 M in that same
time span.
To the statistically impaired,
these predictions raise more ques
ti<*ns than they answer. How did the
USGS come up with these probabil
ities anyway? And why juggle the
numbers around so much if they
end up being about the same—to
the average observer —as the old
numbers? Does this incremental
change really affect how we prepare
for earthquakes?
The time span itself makes one
doubtful: 30 years? Is that the small
est time span for which their figures
have any predictive merit?
Why the Number Juggling?
It turns out there are good rea
♦stfhs for the number juggling. For
•example, the new predictions
-address the chance of a 6.7 M quake
-or greater because the Northridge
.quake, which caused $20 to $40 bil-
Ljion of damage in 1994, was a 6.7 M
"quake. Since we have to worry
[about quakes that are smaller than
'>7.0 M. especially in urban areas, the
*\jSGS decided to include such
•quakes in their predictions.

The revised report also took into
account five major Bay Area faults
that were omitted from previous
analyses due to lack of data. The
1990 predictions considered only
the probability of a quake on the
two biggest faults: San Andreas and
the Hayward-Rodgers Creek Fault.
The 1999 predictions include
probabilities of producing a 6.7 M
quake or greater over the next 30
years not only for San Andreas (21
percent) and Hay ward (32 percent),
but for five other major faults in the
Bay area as well: Calaveras, 18 per
cent; San Gregorio, 10 percent;
Concord-Green Valley, 6 percent;
Mount Diablo, 4 percent; and
Greenville, 6 percent. They also
include a 9-percent chance of
quakes that aren't on known faults
(because Loma Prieta was itself on
an unknown fault).
With more faults in the calcula
tions, one would expect a huge
jump in the probability predictions,
but there isn't one because the 1999
predictions were modified in other
ways that served to decrease the
probabilities.
Most notably, the 1990 estimates
ignored the fact that when a quake
occurs, it relieves some of the stress
on all the faults in the region. The
1906 quake, for example, was fol
lowed by many years of relative
earthquake silence.
Indeed, for the 60 years after the
1906 quake, there were only five
quakes greater than 5.5 on the
Richter scale, compared to 30 such
quakes during the 40 years before
the 1906 quake. Because this effect
of the 1906 quake is still present, the
new predictions take it into account.
In the past, probability predic
tions were based on a model that
had no "memory" of when the last
quake occurred. The only informa
tion needed to calculate the proba
bility of a big quake was how often,
on average, quakes of a certain

magnitude are known to have hap
pened in the past, and therefore
how often they can be expected to
occur in the future. The model
assumed that earthquakes occur
randomly in time.
This model was useful, but did
not match the physical reality:
earthquakes do not occur randomly
in time; their occurrence is related
to the stress along the fault segment.
When stress is released, the proba
bility of a quake drops; when stress
has not been released fora while,
the probability of a quake increases.
Recognizing this, the new proba
bility predictions used statistical
methods that account for the
buildup and release of stress. Pic
ture a rope pulling a spring attached
to a heavy object. As the spring gets
extended, the tension in it increases.
Every once in a while, the spring is
able to pull the solid object forward,
releasing tension. Sometimes a lot
of tension is released, sometimes
only a little.
The period of time between
releases of tension has a certain pre
dictability. The USGS's new model
takes into account not only the pre
dictability of the time periods
between quakes, but also the size of
the prior quake (i.e., the amount of
stress released).
Because the new predictions for
the years 21KK) to 2030 depend on
when the last quake occurred, they
are "conditional" probabilities.
They assume that no 6.7 M or
greater quake has occurred before
January 1, 1999. As soon as such a
quake occurs, the probability pre
dictions will necessarily change.
Why 30 Years?
Why provide predictions fora 30
year time period? On the various
Bay Area fault systems, the average
time spans between 6.7 M or greater
quakes are in the 200 to 1,200 year

range. That is, we are talking geo
logic time—not human time.
The choice of a 30-year time
span for prediction was based on
the desire to frame the study's
results in terms that can affect poli
cy making today. Since most people
care more about what will happen
in their own lifetimes, it doesn't
make sense to provide probability
figures for the next century or two.
If that's true, why not give annu
al probabilities? Within the USGS
report, there are, in fact, some fig
ures for the historical annual rate of
earthquakes. Since 1942, the annual
probability of earthquakes magni
tude 6.7 M or greater has wavered
between 1.1 percent and 1.7 per
cent. Not very scary sounding num
bers.
Before the 1906 quake, that rate
was in the 4- to 8-percent range as
the stresses built up to the Big One.
But from 1850-1905, the population
of California was minuscule com
pared to today.
Now, with heavier developments
throughout the Bay Area, one
quake such as those preceding the
1906 quake could cause enormous
economic losses, not to mention
losses of life. Annualized probabili
ties, which seem to minimize the
risk of a Big One, are therefore not
as useful as the 30-year predictions
the USGS has provided.
Still Skeptical?
Despite these explanations for
USGS's predictions, there remain
several bases for skepticism about
their validity. One troubling issue is
the slipping of faults in the absence
of earthquakes.
Apparently, "surface creep,"
which involves movement along a
fault without a quake happening, is
somewhat common on some of the
Bay-Area faults, including the Hay
ward, Calaveras and Concord-

Green Valley Faults.
While the new predictions try to
take this into account, (it is one rea
son for the huge decrease in the
probability of a big one on the Hay
ward fault —from 50 percent to 32
percent) it is not clear whether this
is valid since little is known about
how creep affects the size and fre
quency of quakes.
A second issue that raises ques
tions about the report is the use of
"experts' weighted aggregate" pre
dictions. Basically, after the scien
tists crunched the numbers using
several statistical models, they
showed the results to experts who
"weighted" the models depending
on their appropriateness to the data
available and the appropriateness
of the model to the particular fault.
The report gives little explana
tion as to how the weights affect the
probability predictions for any par
ticular fault or for the aggregate. It
seems that clear, logical statistical
analyses were fuzzed up for reasons

that haven't been stated, increasing
the level of uncertainty in the
result.
How Does this Affect Us?
Ultimately, there is still the
question of whether incremental
changes in USGS's predictions
have any real impact on how we, as
a society, make both personal and
policy decisions about how to pre
pare for the Big One.
Perhaps occasional announce
ments of an increased risk of earth
quakes will convince a few people
to go buy earthquake supplies to
keep in the back yard, or to finally
get their houses bolted to their
foundations.
One hopes that USGS's years of
federally funded earthquake
research will, one day, have a
greater payoff —like actually
enabling us to predict specific
quakes and save thousands of lives
all aiound the world.

•Ten years after the Loma Prieta
earthquake caused enormous dam
age in the Bay Area in 1989, the
results of studying that quake are in:
things are worse than we thought.
Last week, the U.S. Geological
Survey revised its predictions for
the probability that the Big One will
hit somewhere in the San Francisco
Bay region. In 1990, they said there
was a 67-percent chance of a quake
greater than 7.0 M (moments of
magnitude) hitting the region by
2020.
Now, 10 years later, the predic
tion is a 70-percent chance of a 6.7-
M quake or greater before the year
2030, and an 80-percent chance of a
quake 6.0- to 6.6 M in that same
time span.
To the statistically impaired,
these predictions raise more ques
ti<*ns than they answer. How did the
USGS come up with these probabil
ities anyway? And why juggle the
numbers around so much if they
end up being about the same—to
the average observer —as the old
numbers? Does this incremental
change really affect how we prepare
for earthquakes?
The time span itself makes one
doubtful: 30 years? Is that the small
est time span for which their figures
have any predictive merit?
Why the Number Juggling?
It turns out there are good rea
♦stfhs for the number juggling. For
•example, the new predictions
-address the chance of a 6.7 M quake
-or greater because the Northridge
.quake, which caused $20 to $40 bil-
Ljion of damage in 1994, was a 6.7 M
"quake. Since we have to worry
[about quakes that are smaller than
'>7.0 M. especially in urban areas, the
*\jSGS decided to include such
•quakes in their predictions.

The revised report also took into
account five major Bay Area faults
that were omitted from previous
analyses due to lack of data. The
1990 predictions considered only
the probability of a quake on the
two biggest faults: San Andreas and
the Hayward-Rodgers Creek Fault.
The 1999 predictions include
probabilities of producing a 6.7 M
quake or greater over the next 30
years not only for San Andreas (21
percent) and Hay ward (32 percent),
but for five other major faults in the
Bay area as well: Calaveras, 18 per
cent; San Gregorio, 10 percent;
Concord-Green Valley, 6 percent;
Mount Diablo, 4 percent; and
Greenville, 6 percent. They also
include a 9-percent chance of
quakes that aren't on known faults
(because Loma Prieta was itself on
an unknown fault).
With more faults in the calcula
tions, one would expect a huge
jump in the probability predictions,
but there isn't one because the 1999
predictions were modified in other
ways that served to decrease the
probabilities.
Most notably, the 1990 estimates
ignored the fact that when a quake
occurs, it relieves some of the stress
on all the faults in the region. The
1906 quake, for example, was fol
lowed by many years of relative
earthquake silence.
Indeed, for the 60 years after the
1906 quake, there were only five
quakes greater than 5.5 on the
Richter scale, compared to 30 such
quakes during the 40 years before
the 1906 quake. Because this effect
of the 1906 quake is still present, the
new predictions take it into account.
In the past, probability predic
tions were based on a model that
had no "memory" of when the last
quake occurred. The only informa
tion needed to calculate the proba
bility of a big quake was how often,
on average, quakes of a certain

magnitude are known to have hap
pened in the past, and therefore
how often they can be expected to
occur in the future. The model
assumed that earthquakes occur
randomly in time.
This model was useful, but did
not match the physical reality:
earthquakes do not occur randomly
in time; their occurrence is related
to the stress along the fault segment.
When stress is released, the proba
bility of a quake drops; when stress
has not been released fora while,
the probability of a quake increases.
Recognizing this, the new proba
bility predictions used statistical
methods that account for the
buildup and release of stress. Pic
ture a rope pulling a spring attached
to a heavy object. As the spring gets
extended, the tension in it increases.
Every once in a while, the spring is
able to pull the solid object forward,
releasing tension. Sometimes a lot
of tension is released, sometimes
only a little.
The period of time between
releases of tension has a certain pre
dictability. The USGS's new model
takes into account not only the pre
dictability of the time periods
between quakes, but also the size of
the prior quake (i.e., the amount of
stress released).
Because the new predictions for
the years 21KK) to 2030 depend on
when the last quake occurred, they
are "conditional" probabilities.
They assume that no 6.7 M or
greater quake has occurred before
January 1, 1999. As soon as such a
quake occurs, the probability pre
dictions will necessarily change.
Why 30 Years?
Why provide predictions fora 30
year time period? On the various
Bay Area fault systems, the average
time spans between 6.7 M or greater
quakes are in the 200 to 1,200 year

range. That is, we are talking geo
logic time—not human time.
The choice of a 30-year time
span for prediction was based on
the desire to frame the study's
results in terms that can affect poli
cy making today. Since most people
care more about what will happen
in their own lifetimes, it doesn't
make sense to provide probability
figures for the next century or two.
If that's true, why not give annu
al probabilities? Within the USGS
report, there are, in fact, some fig
ures for the historical annual rate of
earthquakes. Since 1942, the annual
probability of earthquakes magni
tude 6.7 M or greater has wavered
between 1.1 percent and 1.7 per
cent. Not very scary sounding num
bers.
Before the 1906 quake, that rate
was in the 4- to 8-percent range as
the stresses built up to the Big One.
But from 1850-1905, the population
of California was minuscule com
pared to today.
Now, with heavier developments
throughout the Bay Area, one
quake such as those preceding the
1906 quake could cause enormous
economic losses, not to mention
losses of life. Annualized probabili
ties, which seem to minimize the
risk of a Big One, are therefore not
as useful as the 30-year predictions
the USGS has provided.
Still Skeptical?
Despite these explanations for
USGS's predictions, there remain
several bases for skepticism about
their validity. One troubling issue is
the slipping of faults in the absence
of earthquakes.
Apparently, "surface creep,"
which involves movement along a
fault without a quake happening, is
somewhat common on some of the
Bay-Area faults, including the Hay
ward, Calaveras and Concord-

Green Valley Faults.
While the new predictions try to
take this into account, (it is one rea
son for the huge decrease in the
probability of a big one on the Hay
ward fault —from 50 percent to 32
percent) it is not clear whether this
is valid since little is known about
how creep affects the size and fre
quency of quakes.
A second issue that raises ques
tions about the report is the use of
"experts' weighted aggregate" pre
dictions. Basically, after the scien
tists crunched the numbers using
several statistical models, they
showed the results to experts who
"weighted" the models depending
on their appropriateness to the data
available and the appropriateness
of the model to the particular fault.
The report gives little explana
tion as to how the weights affect the
probability predictions for any par
ticular fault or for the aggregate. It
seems that clear, logical statistical
analyses were fuzzed up for reasons

that haven't been stated, increasing
the level of uncertainty in the
result.
How Does this Affect Us?
Ultimately, there is still the
question of whether incremental
changes in USGS's predictions
have any real impact on how we, as
a society, make both personal and
policy decisions about how to pre
pare for the Big One.
Perhaps occasional announce
ments of an increased risk of earth
quakes will convince a few people
to go buy earthquake supplies to
keep in the back yard, or to finally
get their houses bolted to their
foundations.
One hopes that USGS's years of
federally funded earthquake
research will, one day, have a
greater payoff —like actually
enabling us to predict specific
quakes and save thousands of lives
all aiound the world.

•Ten years after the Loma Prieta
earthquake caused enormous dam
age in the Bay Area in 1989, the
results of studying that quake are in:
things are worse than we thought.
Last week, the U.S. Geological
Survey revised its predictions for
the probability that the Big One will
hit somewhere in the San Francisco
Bay region. In 1990, they said there
was a 67-percent chance of a quake
greater than 7.0 M (moments of
magnitude) hitting the region by
2020.
Now, 10 years later, the predic
tion is a 70-percent chance of a 6.7-
M quake or greater before the year
2030, and an 80-percent chance of a
quake 6.0- to 6.6 M in that same
time span.
To the statistically impaired,
these predictions raise more ques
ti<*ns than they answer. How did the
USGS come up with these probabil
ities anyway? And why juggle the
numbers around so much if they
end up being about the same—to
the average observer —as the old
numbers? Does this incremental
change really affect how we prepare
for earthquakes?
The time span itself makes one
doubtful: 30 years? Is that the small
est time span for which their figures
have any predictive merit?
Why the Number Juggling?
It turns out there are good rea
♦stfhs for the number juggling. For
•example, the new predictions
-address the chance of a 6.7 M quake
-or greater because the Northridge
.quake, which caused $20 to $40 bil-
Ljion of damage in 1994, was a 6.7 M
"quake. Since we have to worry
[about quakes that are smaller than
'>7.0 M. especially in urban areas, the
*\jSGS decided to include such
•quakes in their predictions.

The revised report also took into
account five major Bay Area faults
that were omitted from previous
analyses due to lack of data. The
1990 predictions considered only
the probability of a quake on the
two biggest faults: San Andreas and
the Hayward-Rodgers Creek Fault.
The 1999 predictions include
probabilities of producing a 6.7 M
quake or greater over the next 30
years not only for San Andreas (21
percent) and Hay ward (32 percent),
but for five other major faults in the
Bay area as well: Calaveras, 18 per
cent; San Gregorio, 10 percent;
Concord-Green Valley, 6 percent;
Mount Diablo, 4 percent; and
Greenville, 6 percent. They also
include a 9-percent chance of
quakes that aren't on known faults
(because Loma Prieta was itself on
an unknown fault).
With more faults in the calcula
tions, one would expect a huge
jump in the probability predictions,
but there isn't one because the 1999
predictions were modified in other
ways that served to decrease the
probabilities.
Most notably, the 1990 estimates
ignored the fact that when a quake
occurs, it relieves some of the stress
on all the faults in the region. The
1906 quake, for example, was fol
lowed by many years of relative
earthquake silence.
Indeed, for the 60 years after the
1906 quake, there were only five
quakes greater than 5.5 on the
Richter scale, compared to 30 such
quakes during the 40 years before
the 1906 quake. Because this effect
of the 1906 quake is still present, the
new predictions take it into account.
In the past, probability predic
tions were based on a model that
had no "memory" of when the last
quake occurred. The only informa
tion needed to calculate the proba
bility of a big quake was how often,
on average, quakes of a certain

magnitude are known to have hap
pened in the past, and therefore
how often they can be expected to
occur in the future. The model
assumed that earthquakes occur
randomly in time.
This model was useful, but did
not match the physical reality:
earthquakes do not occur randomly
in time; their occurrence is related
to the stress along the fault segment.
When stress is released, the proba
bility of a quake drops; when stress
has not been released fora while,
the probability of a quake increases.
Recognizing this, the new proba
bility predictions used statistical
methods that account for the
buildup and release of stress. Pic
ture a rope pulling a spring attached
to a heavy object. As the spring gets
extended, the tension in it increases.
Every once in a while, the spring is
able to pull the solid object forward,
releasing tension. Sometimes a lot
of tension is released, sometimes
only a little.
The period of time between
releases of tension has a certain pre
dictability. The USGS's new model
takes into account not only the pre
dictability of the time periods
between quakes, but also the size of
the prior quake (i.e., the amount of
stress released).
Because the new predictions for
the years 21KK) to 2030 depend on
when the last quake occurred, they
are "conditional" probabilities.
They assume that no 6.7 M or
greater quake has occurred before
January 1, 1999. As soon as such a
quake occurs, the probability pre
dictions will necessarily change.
Why 30 Years?
Why provide predictions fora 30
year time period? On the various
Bay Area fault systems, the average
time spans between 6.7 M or greater
quakes are in the 200 to 1,200 year

range. That is, we are talking geo
logic time—not human time.
The choice of a 30-year time
span for prediction was based on
the desire to frame the study's
results in terms that can affect poli
cy making today. Since most people
care more about what will happen
in their own lifetimes, it doesn't
make sense to provide probability
figures for the next century or two.
If that's true, why not give annu
al probabilities? Within the USGS
report, there are, in fact, some fig
ures for the historical annual rate of
earthquakes. Since 1942, the annual
probability of earthquakes magni
tude 6.7 M or greater has wavered
between 1.1 percent and 1.7 per
cent. Not very scary sounding num
bers.
Before the 1906 quake, that rate
was in the 4- to 8-percent range as
the stresses built up to the Big One.
But from 1850-1905, the population
of California was minuscule com
pared to today.
Now, with heavier developments
throughout the Bay Area, one
quake such as those preceding the
1906 quake could cause enormous
economic losses, not to mention
losses of life. Annualized probabili
ties, which seem to minimize the
risk of a Big One, are therefore not
as useful as the 30-year predictions
the USGS has provided.
Still Skeptical?
Despite these explanations for
USGS's predictions, there remain
several bases for skepticism about
their validity. One troubling issue is
the slipping of faults in the absence
of earthquakes.
Apparently, "surface creep,"
which involves movement along a
fault without a quake happening, is
somewhat common on some of the
Bay-Area faults, including the Hay
ward, Calaveras and Concord-

Green Valley Faults.
While the new predictions try to
take this into account, (it is one rea
son for the huge decrease in the
probability of a big one on the Hay
ward fault —from 50 percent to 32
percent) it is not clear whether this
is valid since little is known about
how creep affects the size and fre
quency of quakes.
A second issue that raises ques
tions about the report is the use of
"experts' weighted aggregate" pre
dictions. Basically, after the scien
tists crunched the numbers using
several statistical models, they
showed the results to experts who
"weighted" the models depending
on their appropriateness to the data
available and the appropriateness
of the model to the particular fault.
The report gives little explana
tion as to how the weights affect the
probability predictions for any par
ticular fault or for the aggregate. It
seems that clear, logical statistical
analyses were fuzzed up for reasons

that haven't been stated, increasing
the level of uncertainty in the
result.
How Does this Affect Us?
Ultimately, there is still the
question of whether incremental
changes in USGS's predictions
have any real impact on how we, as
a society, make both personal and
policy decisions about how to pre
pare for the Big One.
Perhaps occasional announce
ments of an increased risk of earth
quakes will convince a few people
to go buy earthquake supplies to
keep in the back yard, or to finally
get their houses bolted to their
foundations.
One hopes that USGS's years of
federally funded earthquake
research will, one day, have a
greater payoff —like actually
enabling us to predict specific
quakes and save thousands of lives
all aiound the world.

•Ten years after the Loma Prieta
earthquake caused enormous dam
age in the Bay Area in 1989, the
results of studying that quake are in:
things are worse than we thought.
Last week, the U.S. Geological
Survey revised its predictions for
the probability that the Big One will
hit somewhere in the San Francisco
Bay region. In 1990, they said there
was a 67-percent chance of a quake
greater than 7.0 M (moments of
magnitude) hitting the region by
2020.
Now, 10 years later, the predic
tion is a 70-percent chance of a 6.7-
M quake or greater before the year
2030, and an 80-percent chance of a
quake 6.0- to 6.6 M in that same
time span.
To the statistically impaired,
these predictions raise more ques
ti<*ns than they answer. How did the
USGS come up with these probabil
ities anyway? And why juggle the
numbers around so much if they
end up being about the same—to
the average observer —as the old
numbers? Does this incremental
change really affect how we prepare
for earthquakes?
The time span itself makes one
doubtful: 30 years? Is that the small
est time span for which their figures
have any predictive merit?
Why the Number Juggling?
It turns out there are good rea
♦stfhs for the number juggling. For
•example, the new predictions
-address the chance of a 6.7 M quake
-or greater because the Northridge
.quake, which caused $20 to $40 bil-
Ljion of damage in 1994, was a 6.7 M
"quake. Since we have to worry
[about quakes that are smaller than
'>7.0 M. especially in urban areas, the
*\jSGS decided to include such
•quakes in their predictions.

The revised report also took into
account five major Bay Area faults
that were omitted from previous
analyses due to lack of data. The
1990 predictions considered only
the probability of a quake on the
two biggest faults: San Andreas and
the Hayward-Rodgers Creek Fault.
The 1999 predictions include
probabilities of producing a 6.7 M
quake or greater over the next 30
years not only for San Andreas (21
percent) and Hay ward (32 percent),
but for five other major faults in the
Bay area as well: Calaveras, 18 per
cent; San Gregorio, 10 percent;
Concord-Green Valley, 6 percent;
Mount Diablo, 4 percent; and
Greenville, 6 percent. They also
include a 9-percent chance of
quakes that aren't on known faults
(because Loma Prieta was itself on
an unknown fault).
With more faults in the calcula
tions, one would expect a huge
jump in the probability predictions,
but there isn't one because the 1999
predictions were modified in other
ways that served to decrease the
probabilities.
Most notably, the 1990 estimates
ignored the fact that when a quake
occurs, it relieves some of the stress
on all the faults in the region. The
1906 quake, for example, was fol
lowed by many years of relative
earthquake silence.
Indeed, for the 60 years after the
1906 quake, there were only five
quakes greater than 5.5 on the
Richter scale, compared to 30 such
quakes during the 40 years before
the 1906 quake. Because this effect
of the 1906 quake is still present, the
new predictions take it into account.
In the past, probability predic
tions were based on a model that
had no "memory" of when the last
quake occurred. The only informa
tion needed to calculate the proba
bility of a big quake was how often,
on average, quakes of a certain

magnitude are known to have hap
pened in the past, and therefore
how often they can be expected to
occur in the future. The model
assumed that earthquakes occur
randomly in time.
This model was useful, but did
not match the physical reality:
earthquakes do not occur randomly
in time; their occurrence is related
to the stress along the fault segment.
When stress is released, the proba
bility of a quake drops; when stress
has not been released fora while,
the probability of a quake increases.
Recognizing this, the new proba
bility predictions used statistical
methods that account for the
buildup and release of stress. Pic
ture a rope pulling a spring attached
to a heavy object. As the spring gets
extended, the tension in it increases.
Every once in a while, the spring is
able to pull the solid object forward,
releasing tension. Sometimes a lot
of tension is released, sometimes
only a little.
The period of time between
releases of tension has a certain pre
dictability. The USGS's new model
takes into account not only the pre
dictability of the time periods
between quakes, but also the size of
the prior quake (i.e., the amount of
stress released).
Because the new predictions for
the years 21KK) to 2030 depend on
when the last quake occurred, they
are "conditional" probabilities.
They assume that no 6.7 M or
greater quake has occurred before
January 1, 1999. As soon as such a
quake occurs, the probability pre
dictions will necessarily change.
Why 30 Years?
Why provide predictions fora 30
year time period? On the various
Bay Area fault systems, the average
time spans between 6.7 M or greater
quakes are in the 200 to 1,200 year

range. That is, we are talking geo
logic time—not human time.
The choice of a 30-year time
span for prediction was based on
the desire to frame the study's
results in terms that can affect poli
cy making today. Since most people
care more about what will happen
in their own lifetimes, it doesn't
make sense to provide probability
figures for the next century or two.
If that's true, why not give annu
al probabilities? Within the USGS
report, there are, in fact, some fig
ures for the historical annual rate of
earthquakes. Since 1942, the annual
probability of earthquakes magni
tude 6.7 M or greater has wavered
between 1.1 percent and 1.7 per
cent. Not very scary sounding num
bers.
Before the 1906 quake, that rate
was in the 4- to 8-percent range as
the stresses built up to the Big One.
But from 1850-1905, the population
of California was minuscule com
pared to today.
Now, with heavier developments
throughout the Bay Area, one
quake such as those preceding the
1906 quake could cause enormous
economic losses, not to mention
losses of life. Annualized probabili
ties, which seem to minimize the
risk of a Big One, are therefore not
as useful as the 30-year predictions
the USGS has provided.
Still Skeptical?
Despite these explanations for
USGS's predictions, there remain
several bases for skepticism about
their validity. One troubling issue is
the slipping of faults in the absence
of earthquakes.
Apparently, "surface creep,"
which involves movement along a
fault without a quake happening, is
somewhat common on some of the
Bay-Area faults, including the Hay
ward, Calaveras and Concord-

Green Valley Faults.
While the new predictions try to
take this into account, (it is one rea
son for the huge decrease in the
probability of a big one on the Hay
ward fault —from 50 percent to 32
percent) it is not clear whether this
is valid since little is known about
how creep affects the size and fre
quency of quakes.
A second issue that raises ques
tions about the report is the use of
"experts' weighted aggregate" pre
dictions. Basically, after the scien
tists crunched the numbers using
several statistical models, they
showed the results to experts who
"weighted" the models depending
on their appropriateness to the data
available and the appropriateness
of the model to the particular fault.
The report gives little explana
tion as to how the weights affect the
probability predictions for any par
ticular fault or for the aggregate. It
seems that clear, logical statistical
analyses were fuzzed up for reasons

that haven't been stated, increasing
the level of uncertainty in the
result.
How Does this Affect Us?
Ultimately, there is still the
question of whether incremental
changes in USGS's predictions
have any real impact on how we, as
a society, make both personal and
policy decisions about how to pre
pare for the Big One.
Perhaps occasional announce
ments of an increased risk of earth
quakes will convince a few people
to go buy earthquake supplies to
keep in the back yard, or to finally
get their houses bolted to their
foundations.
One hopes that USGS's years of
federally funded earthquake
research will, one day, have a
greater payoff —like actually
enabling us to predict specific
quakes and save thousands of lives
all aiound the world.

•Ten years after the Loma Prieta
earthquake caused enormous dam
age in the Bay Area in 1989, the
results of studying that quake are in:
things are worse than we thought.
Last week, the U.S. Geological
Survey revised its predictions for
the probability that the Big One will
hit somewhere in the San Francisco
Bay region. In 1990, they said there
was a 67-percent chance of a quake
greater than 7.0 M (moments of
magnitude) hitting the region by
2020.
Now, 10 years later, the predic
tion is a 70-percent chance of a 6.7-
M quake or greater before the year
2030, and an 80-percent chance of a
quake 6.0- to 6.6 M in that same
time span.
To the statistically impaired,
these predictions raise more ques
ti<*ns than they answer. How did the
USGS come up with these probabil
ities anyway? And why juggle the
numbers around so much if they
end up being about the same—to
the average observer —as the old
numbers? Does this incremental
change really affect how we prepare
for earthquakes?
The time span itself makes one
doubtful: 30 years? Is that the small
est time span for which their figures
have any predictive merit?
Why the Number Juggling?
It turns out there are good rea
♦stfhs for the number juggling. For
•example, the new predictions
-address the chance of a 6.7 M quake
-or greater because the Northridge
.quake, which caused $20 to $40 bil-
Ljion of damage in 1994, was a 6.7 M
"quake. Since we have to worry
[about quakes that are smaller than
'>7.0 M. especially in urban areas, the
*\jSGS decided to include such
•quakes in their predictions.

The revised report also took into
account five major Bay Area faults
that were omitted from previous
analyses due to lack of data. The
1990 predictions considered only
the probability of a quake on the
two biggest faults: San Andreas and
the Hayward-Rodgers Creek Fault.
The 1999 predictions include
probabilities of producing a 6.7 M
quake or greater over the next 30
years not only for San Andreas (21
percent) and Hay ward (32 percent),
but for five other major faults in the
Bay area as well: Calaveras, 18 per
cent; San Gregorio, 10 percent;
Concord-Green Valley, 6 percent;
Mount Diablo, 4 percent; and
Greenville, 6 percent. They also
include a 9-percent chance of
quakes that aren't on known faults
(because Loma Prieta was itself on
an unknown fault).
With more faults in the calcula
tions, one would expect a huge
jump in the probability predictions,
but there isn't one because the 1999
predictions were modified in other
ways that served to decrease the
probabilities.
Most notably, the 1990 estimates
ignored the fact that when a quake
occurs, it relieves some of the stress
on all the faults in the region. The
1906 quake, for example, was fol
lowed by many years of relative
earthquake silence.
Indeed, for the 60 years after the
1906 quake, there were only five
quakes greater than 5.5 on the
Richter scale, compared to 30 such
quakes during the 40 years before
the 1906 quake. Because this effect
of the 1906 quake is still present, the
new predictions take it into account.
In the past, probability predic
tions were based on a model that
had no "memory" of when the last
quake occurred. The only informa
tion needed to calculate the proba
bility of a big quake was how often,
on average, quakes of a certain

magnitude are known to have hap
pened in the past, and therefore
how often they can be expected to
occur in the future. The model
assumed that earthquakes occur
randomly in time.
This model was useful, but did
not match the physical reality:
earthquakes do not occur randomly
in time; their occurrence is related
to the stress along the fault segment.
When stress is released, the proba
bility of a quake drops; when stress
has not been released fora while,
the probability of a quake increases.
Recognizing this, the new proba
bility predictions used statistical
methods that account for the
buildup and release of stress. Pic
ture a rope pulling a spring attached
to a heavy object. As the spring gets
extended, the tension in it increases.
Every once in a while, the spring is
able to pull the solid object forward,
releasing tension. Sometimes a lot
of tension is released, sometimes
only a little.
The period of time between
releases of tension has a certain pre
dictability. The USGS's new model
takes into account not only the pre
dictability of the time periods
between quakes, but also the size of
the prior quake (i.e., the amount of
stress released).
Because the new predictions for
the years 21KK) to 2030 depend on
when the last quake occurred, they
are "conditional" probabilities.
They assume that no 6.7 M or
greater quake has occurred before
January 1, 1999. As soon as such a
quake occurs, the probability pre
dictions will necessarily change.
Why 30 Years?
Why provide predictions fora 30
year time period? On the various
Bay Area fault systems, the average
time spans between 6.7 M or greater
quakes are in the 200 to 1,200 year

range. That is, we are talking geo
logic time—not human time.
The choice of a 30-year time
span for prediction was based on
the desire to frame the study's
results in terms that can affect poli
cy making today. Since most people
care more about what will happen
in their own lifetimes, it doesn't
make sense to provide probability
figures for the next century or two.
If that's true, why not give annu
al probabilities? Within the USGS
report, there are, in fact, some fig
ures for the historical annual rate of
earthquakes. Since 1942, the annual
probability of earthquakes magni
tude 6.7 M or greater has wavered
between 1.1 percent and 1.7 per
cent. Not very scary sounding num
bers.
Before the 1906 quake, that rate
was in the 4- to 8-percent range as
the stresses built up to the Big One.
But from 1850-1905, the population
of California was minuscule com
pared to today.
Now, with heavier developments
throughout the Bay Area, one
quake such as those preceding the
1906 quake could cause enormous
economic losses, not to mention
losses of life. Annualized probabili
ties, which seem to minimize the
risk of a Big One, are therefore not
as useful as the 30-year predictions
the USGS has provided.
Still Skeptical?
Despite these explanations for
USGS's predictions, there remain
several bases for skepticism about
their validity. One troubling issue is
the slipping of faults in the absence
of earthquakes.
Apparently, "surface creep,"
which involves movement along a
fault without a quake happening, is
somewhat common on some of the
Bay-Area faults, including the Hay
ward, Calaveras and Concord-

Green Valley Faults.
While the new predictions try to
take this into account, (it is one rea
son for the huge decrease in the
probability of a big one on the Hay
ward fault —from 50 percent to 32
percent) it is not clear whether this
is valid since little is known about
how creep affects the size and fre
quency of quakes.
A second issue that raises ques
tions about the report is the use of
"experts' weighted aggregate" pre
dictions. Basically, after the scien
tists crunched the numbers using
several statistical models, they
showed the results to experts who
"weighted" the models depending
on their appropriateness to the data
available and the appropriateness
of the model to the particular fault.
The report gives little explana
tion as to how the weights affect the
probability predictions for any par
ticular fault or for the aggregate. It
seems that clear, logical statistical
analyses were fuzzed up for reasons

that haven't been stated, increasing
the level of uncertainty in the
result.
How Does this Affect Us?
Ultimately, there is still the
question of whether incremental
changes in USGS's predictions
have any real impact on how we, as
a society, make both personal and
policy decisions about how to pre
pare for the Big One.
Perhaps occasional announce
ments of an increased risk of earth
quakes will convince a few people
to go buy earthquake supplies to
keep in the back yard, or to finally
get their houses bolted to their
foundations.
One hopes that USGS's years of
federally funded earthquake
research will, one day, have a
greater payoff —like actually
enabling us to predict specific
quakes and save thousands of lives
all aiound the world.

•Ten years after the Loma Prieta
earthquake caused enormous dam
age in the Bay Area in 1989, the
results of studying that quake are in:
things are worse than we thought.
Last week, the U.S. Geological
Survey revised its predictions for
the probability that the Big One will
hit somewhere in the San Francisco
Bay region. In 1990, they said there
was a 67-percent chance of a quake
greater than 7.0 M (moments of
magnitude) hitting the region by
2020.
Now, 10 years later, the predic
tion is a 70-percent chance of a 6.7-
M quake or greater before the year
2030, and an 80-percent chance of a
quake 6.0- to 6.6 M in that same
time span.
To the statistically impaired,
these predictions raise more ques
ti<*ns than they answer. How did the
USGS come up with these probabil
ities anyway? And why juggle the
numbers around so much if they
end up being about the same—to
the average observer —as the old
numbers? Does this incremental
change really affect how we prepare
for earthquakes?
The time span itself makes one
doubtful: 30 years? Is that the small
est time span for which their figures
have any predictive merit?
Why the Number Juggling?
It turns out there are good rea
♦stfhs for the number juggling. For
•example, the new predictions
-address the chance of a 6.7 M quake
-or greater because the Northridge
.quake, which caused $20 to $40 bil-
Ljion of damage in 1994, was a 6.7 M
"quake. Since we have to worry
[about quakes that are smaller than
'>7.0 M. especially in urban areas, the
*\jSGS decided to include such
•quakes in their predictions.

The revised report also took into
account five major Bay Area faults
that were omitted from previous
analyses due to lack of data. The
1990 predictions considered only
the probability of a quake on the
two biggest faults: San Andreas and
the Hayward-Rodgers Creek Fault.
The 1999 predictions include
probabilities of producing a 6.7 M
quake or greater over the next 30
years not only for San Andreas (21
percent) and Hay ward (32 percent),
but for five other major faults in the
Bay area as well: Calaveras, 18 per
cent; San Gregorio, 10 percent;
Concord-Green Valley, 6 percent;
Mount Diablo, 4 percent; and
Greenville, 6 percent. They also
include a 9-percent chance of
quakes that aren't on known faults
(because Loma Prieta was itself on
an unknown fault).
With more faults in the calcula
tions, one would expect a huge
jump in the probability predictions,
but there isn't one because the 1999
predictions were modified in other
ways that served to decrease the
probabilities.
Most notably, the 1990 estimates
ignored the fact that when a quake
occurs, it relieves some of the stress
on all the faults in the region. The
1906 quake, for example, was fol
lowed by many years of relative
earthquake silence.
Indeed, for the 60 years after the
1906 quake, there were only five
quakes greater than 5.5 on the
Richter scale, compared to 30 such
quakes during the 40 years before
the 1906 quake. Because this effect
of the 1906 quake is still present, the
new predictions take it into account.
In the past, probability predic
tions were based on a model that
had no "memory" of when the last
quake occurred. The only informa
tion needed to calculate the proba
bility of a big quake was how often,
on average, quakes of a certain

magnitude are known to have hap
pened in the past, and therefore
how often they can be expected to
occur in the future. The model
assumed that earthquakes occur
randomly in time.
This model was useful, but did
not match the physical reality:
earthquakes do not occur randomly
in time; their occurrence is related
to the stress along the fault segment.
When stress is released, the proba
bility of a quake drops; when stress
has not been released fora while,
the probability of a quake increases.
Recognizing this, the new proba
bility predictions used statistical
methods that account for the
buildup and release of stress. Pic
ture a rope pulling a spring attached
to a heavy object. As the spring gets
extended, the tension in it increases.
Every once in a while, the spring is
able to pull the solid object forward,
releasing tension. Sometimes a lot
of tension is released, sometimes
only a little.
The period of time between
releases of tension has a certain pre
dictability. The USGS's new model
takes into account not only the pre
dictability of the time periods
between quakes, but also the size of
the prior quake (i.e., the amount of
stress released).
Because the new predictions for
the years 21KK) to 2030 depend on
when the last quake occurred, they
are "conditional" probabilities.
They assume that no 6.7 M or
greater quake has occurred before
January 1, 1999. As soon as such a
quake occurs, the probability pre
dictions will necessarily change.
Why 30 Years?
Why provide predictions fora 30
year time period? On the various
Bay Area fault systems, the average
time spans between 6.7 M or greater
quakes are in the 200 to 1,200 year

range. That is, we are talking geo
logic time—not human time.
The choice of a 30-year time
span for prediction was based on
the desire to frame the study's
results in terms that can affect poli
cy making today. Since most people
care more about what will happen
in their own lifetimes, it doesn't
make sense to provide probability
figures for the next century or two.
If that's true, why not give annu
al probabilities? Within the USGS
report, there are, in fact, some fig
ures for the historical annual rate of
earthquakes. Since 1942, the annual
probability of earthquakes magni
tude 6.7 M or greater has wavered
between 1.1 percent and 1.7 per
cent. Not very scary sounding num
bers.
Before the 1906 quake, that rate
was in the 4- to 8-percent range as
the stresses built up to the Big One.
But from 1850-1905, the population
of California was minuscule com
pared to today.
Now, with heavier developments
throughout the Bay Area, one
quake such as those preceding the
1906 quake could cause enormous
economic losses, not to mention
losses of life. Annualized probabili
ties, which seem to minimize the
risk of a Big One, are therefore not
as useful as the 30-year predictions
the USGS has provided.
Still Skeptical?
Despite these explanations for
USGS's predictions, there remain
several bases for skepticism about
their validity. One troubling issue is
the slipping of faults in the absence
of earthquakes.
Apparently, "surface creep,"
which involves movement along a
fault without a quake happening, is
somewhat common on some of the
Bay-Area faults, including the Hay
ward, Calaveras and Concord-

Green Valley Faults.
While the new predictions try to
take this into account, (it is one rea
son for the huge decrease in the
probability of a big one on the Hay
ward fault —from 50 percent to 32
percent) it is not clear whether this
is valid since little is known about
how creep affects the size and fre
quency of quakes.
A second issue that raises ques
tions about the report is the use of
"experts' weighted aggregate" pre
dictions. Basically, after the scien
tists crunched the numbers using
several statistical models, they
showed the results to experts who
"weighted" the models depending
on their appropriateness to the data
available and the appropriateness
of the model to the particular fault.
The report gives little explana
tion as to how the weights affect the
probability predictions for any par
ticular fault or for the aggregate. It
seems that clear, logical statistical
analyses were fuzzed up for reasons

that haven't been stated, increasing
the level of uncertainty in the
result.
How Does this Affect Us?
Ultimately, there is still the
question of whether incremental
changes in USGS's predictions
have any real impact on how we, as
a society, make both personal and
policy decisions about how to pre
pare for the Big One.
Perhaps occasional announce
ments of an increased risk of earth
quakes will convince a few people
to go buy earthquake supplies to
keep in the back yard, or to finally
get their houses bolted to their
foundations.
One hopes that USGS's years of
federally funded earthquake
research will, one day, have a
greater payoff —like actually
enabling us to predict specific
quakes and save thousands of lives
all aiound the world.

