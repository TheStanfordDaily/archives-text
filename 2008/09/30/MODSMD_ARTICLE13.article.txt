# Stu's Views
## The 						Insanity of the U.S. News Rankings Stuart Baimel 
### 
This column originally ran on Aug. 28, 2008.
August has been a summer of circuses.
We first had the circus of the
Olympics.Then we have the circus of
the Democratic National Convention, a glo
rified extended commercial that the media
pretends actually matters. But at least those
two happen once every four years.The high
er-education community has to endure annu
al unveilings of the controversial U.S. News
& World Report rankings of undergraduate
universities. Who went down? Who went up?
Which schools "goosed" the rankings? Like
it or not, admissions offices nationwide must
hang on every decimal point, as they know an
improved ranking can drive thousands more
applicants to their school. Just witness the
boon to Penn's acceptance rate after the
school cracked the top five.
It is indeed a shame that these rankings
matter. Besides the obvious conflict-of-inter
est (the conflicting priorities of higher educa
tion and a magazine trying to sell as many is
sues as possible), the U.S. News & World Re
port methodology is grossly flawed. Instead
of measuring outcomes, the report focuses on
incoming statistics — SAT ranges, incoming
students in the top 10 percent of their high
school class and so on."Freshman retention"
weighs heavily, so schools that have difficult
first-year programs like Caltech and MIT
suffer.
The magazine also uses "alumni giving
rate" as part of its methodology; it is unclear
how an alum from the Class of 1980 writing a
$ 1 (K) check has anything to do with the qual
ity of the school in 2008. So although Stan
ford tends to raise twice as much money an
nually as Princeton, they trail in that catego
ry due to the higher number of alumni at
Princeton who donate. Indeed, the rankings
have not been kind to the Farm. Stanford has
trailed behind Harvard, Yale and Princeton
for the past decade — mostly because of the
lower admissions standards for athletes, in
my opinion.
Rankings have become quite an industry.

The undergraduate rankings issue is U.S.
News's top-selling. Forbes, Newsweek and
other publications have created their own
rankings to capitalize on the rankings bo
nanza as undergraduate admissions be
come ever-more competitive. Forbes' at
tempt was particularly weak; a full 25 per
cent of the methodology was based on rate
myprofessors.com. The use of that site cre
ates debilitating selection bias issues that
should — although they won't — de-legit
imize Forbes' rankings entirely. I do not
know a single person who has ever used
that Web site. U.S. News, for its part, does
not make its "Common Data Set," which all
universities submit, publicly available, hin
dering transparency.
Most critics of the U.S. News rankings
argue that it is impossible to rank schools at
all. I disagree. A few years ago, the Wall Street
Journal did a study of professional school
placement. They ranked undergraduate insti
tutions by how many students (as a percent
age of their class) got into lop professional
schools. While professional-school place
ment is only one measure of educational out
come. it is decidedly an important one and
shows which universities professional
schools prefer. But very little data of that sort
is publicly available. More studies that meas
ure placement and outcome would rank
schools far more accurately than the dreamt
up, always-changing methodology of U.S.
News.
"Cross-admits" remain the holy grail of
undergraduate admissions statistics, and the
data most useful to those attempting to rank
schools. For the past several years, Princeton
was top-ranked despite academic studies

showing that students admitted to both I lar
vard and Princeton would choose Harvard
70 percent of the time. Sorting students by
where they actually choose to go to school,
rather than where some profit-seeking mag
azine thinks they should go is the most
meaningful way to rank colleges. Important
ly. it prevents colleges from manipulating
statistics to move up the rankings. Any in
centives that colleges can use to increase
their cross-admit yield, like better financial
aid offers and merit-scholarship programs,
will only benefit students. Instead, the U.S.
News rankings set up a perverse system
where schools like Washington University
in St. Louis will waitlist thousands of good
students in order to lower their acceptance
rate.
It has reached a point where the rankings
have a pernicious effect on the behavior of
schools themselves. Instead of moving to a
more transparent, nonbinding early action
program, or even better, eliminating early
admissions entirely, the rankings incentivize
schools to hold onto their early decision pro
grams and use the waitlist as much as possi
ble. There is even speculation that Stanford's
proposal to increase its class size by 2(H) stu
dents is motivated by Princeton's recent class
size increase, as well as Yale and Chicago's
plans to do so.
Competition and transparency are good
for college admissions. We are, unfortunately,
working under a system that does not allow
the efficiencies of the market to work. A lack
of access to information that really matters,
not "alumni giving rate" and "faculty re
sources" hinders the ability to rank colleges
effectively and most importantly, for high
school students to make the right decision
about their educational future.

Stuart Baimel thinks that the preseason college
football rankings are as distorted as the U.S.
News rankings. Oklahoma and C lemson are
ranked five places too high every year. Send
him your pick for Most Overrated to shaimel
"at" Stanford.edit.

This column originally ran on Aug. 28, 2008.
August has been a summer of circuses.
We first had the circus of the
Olympics.Then we have the circus of
the Democratic National Convention, a glo
rified extended commercial that the media
pretends actually matters. But at least those
two happen once every four years.The high
er-education community has to endure annu
al unveilings of the controversial U.S. News
& World Report rankings of undergraduate
universities. Who went down? Who went up?
Which schools "goosed" the rankings? Like
it or not, admissions offices nationwide must
hang on every decimal point, as they know an
improved ranking can drive thousands more
applicants to their school. Just witness the
boon to Penn's acceptance rate after the
school cracked the top five.
It is indeed a shame that these rankings
matter. Besides the obvious conflict-of-inter
est (the conflicting priorities of higher educa
tion and a magazine trying to sell as many is
sues as possible), the U.S. News & World Re
port methodology is grossly flawed. Instead
of measuring outcomes, the report focuses on
incoming statistics — SAT ranges, incoming
students in the top 10 percent of their high
school class and so on."Freshman retention"
weighs heavily, so schools that have difficult
first-year programs like Caltech and MIT
suffer.
The magazine also uses "alumni giving
rate" as part of its methodology; it is unclear
how an alum from the Class of 1980 writing a
$ 1 (K) check has anything to do with the qual
ity of the school in 2008. So although Stan
ford tends to raise twice as much money an
nually as Princeton, they trail in that catego
ry due to the higher number of alumni at
Princeton who donate. Indeed, the rankings
have not been kind to the Farm. Stanford has
trailed behind Harvard, Yale and Princeton
for the past decade — mostly because of the
lower admissions standards for athletes, in
my opinion.
Rankings have become quite an industry.

The undergraduate rankings issue is U.S.
News's top-selling. Forbes, Newsweek and
other publications have created their own
rankings to capitalize on the rankings bo
nanza as undergraduate admissions be
come ever-more competitive. Forbes' at
tempt was particularly weak; a full 25 per
cent of the methodology was based on rate
myprofessors.com. The use of that site cre
ates debilitating selection bias issues that
should — although they won't — de-legit
imize Forbes' rankings entirely. I do not
know a single person who has ever used
that Web site. U.S. News, for its part, does
not make its "Common Data Set," which all
universities submit, publicly available, hin
dering transparency.
Most critics of the U.S. News rankings
argue that it is impossible to rank schools at
all. I disagree. A few years ago, the Wall Street
Journal did a study of professional school
placement. They ranked undergraduate insti
tutions by how many students (as a percent
age of their class) got into lop professional
schools. While professional-school place
ment is only one measure of educational out
come. it is decidedly an important one and
shows which universities professional
schools prefer. But very little data of that sort
is publicly available. More studies that meas
ure placement and outcome would rank
schools far more accurately than the dreamt
up, always-changing methodology of U.S.
News.
"Cross-admits" remain the holy grail of
undergraduate admissions statistics, and the
data most useful to those attempting to rank
schools. For the past several years, Princeton
was top-ranked despite academic studies

showing that students admitted to both I lar
vard and Princeton would choose Harvard
70 percent of the time. Sorting students by
where they actually choose to go to school,
rather than where some profit-seeking mag
azine thinks they should go is the most
meaningful way to rank colleges. Important
ly. it prevents colleges from manipulating
statistics to move up the rankings. Any in
centives that colleges can use to increase
their cross-admit yield, like better financial
aid offers and merit-scholarship programs,
will only benefit students. Instead, the U.S.
News rankings set up a perverse system
where schools like Washington University
in St. Louis will waitlist thousands of good
students in order to lower their acceptance
rate.
It has reached a point where the rankings
have a pernicious effect on the behavior of
schools themselves. Instead of moving to a
more transparent, nonbinding early action
program, or even better, eliminating early
admissions entirely, the rankings incentivize
schools to hold onto their early decision pro
grams and use the waitlist as much as possi
ble. There is even speculation that Stanford's
proposal to increase its class size by 2(H) stu
dents is motivated by Princeton's recent class
size increase, as well as Yale and Chicago's
plans to do so.
Competition and transparency are good
for college admissions. We are, unfortunately,
working under a system that does not allow
the efficiencies of the market to work. A lack
of access to information that really matters,
not "alumni giving rate" and "faculty re
sources" hinders the ability to rank colleges
effectively and most importantly, for high
school students to make the right decision
about their educational future.

Stuart Baimel thinks that the preseason college
football rankings are as distorted as the U.S.
News rankings. Oklahoma and C lemson are
ranked five places too high every year. Send
him your pick for Most Overrated to shaimel
"at" Stanford.edit.

This column originally ran on Aug. 28, 2008.
August has been a summer of circuses.
We first had the circus of the
Olympics.Then we have the circus of
the Democratic National Convention, a glo
rified extended commercial that the media
pretends actually matters. But at least those
two happen once every four years.The high
er-education community has to endure annu
al unveilings of the controversial U.S. News
& World Report rankings of undergraduate
universities. Who went down? Who went up?
Which schools "goosed" the rankings? Like
it or not, admissions offices nationwide must
hang on every decimal point, as they know an
improved ranking can drive thousands more
applicants to their school. Just witness the
boon to Penn's acceptance rate after the
school cracked the top five.
It is indeed a shame that these rankings
matter. Besides the obvious conflict-of-inter
est (the conflicting priorities of higher educa
tion and a magazine trying to sell as many is
sues as possible), the U.S. News & World Re
port methodology is grossly flawed. Instead
of measuring outcomes, the report focuses on
incoming statistics — SAT ranges, incoming
students in the top 10 percent of their high
school class and so on."Freshman retention"
weighs heavily, so schools that have difficult
first-year programs like Caltech and MIT
suffer.
The magazine also uses "alumni giving
rate" as part of its methodology; it is unclear
how an alum from the Class of 1980 writing a
$ 1 (K) check has anything to do with the qual
ity of the school in 2008. So although Stan
ford tends to raise twice as much money an
nually as Princeton, they trail in that catego
ry due to the higher number of alumni at
Princeton who donate. Indeed, the rankings
have not been kind to the Farm. Stanford has
trailed behind Harvard, Yale and Princeton
for the past decade — mostly because of the
lower admissions standards for athletes, in
my opinion.
Rankings have become quite an industry.

The undergraduate rankings issue is U.S.
News's top-selling. Forbes, Newsweek and
other publications have created their own
rankings to capitalize on the rankings bo
nanza as undergraduate admissions be
come ever-more competitive. Forbes' at
tempt was particularly weak; a full 25 per
cent of the methodology was based on rate
myprofessors.com. The use of that site cre
ates debilitating selection bias issues that
should — although they won't — de-legit
imize Forbes' rankings entirely. I do not
know a single person who has ever used
that Web site. U.S. News, for its part, does
not make its "Common Data Set," which all
universities submit, publicly available, hin
dering transparency.
Most critics of the U.S. News rankings
argue that it is impossible to rank schools at
all. I disagree. A few years ago, the Wall Street
Journal did a study of professional school
placement. They ranked undergraduate insti
tutions by how many students (as a percent
age of their class) got into lop professional
schools. While professional-school place
ment is only one measure of educational out
come. it is decidedly an important one and
shows which universities professional
schools prefer. But very little data of that sort
is publicly available. More studies that meas
ure placement and outcome would rank
schools far more accurately than the dreamt
up, always-changing methodology of U.S.
News.
"Cross-admits" remain the holy grail of
undergraduate admissions statistics, and the
data most useful to those attempting to rank
schools. For the past several years, Princeton
was top-ranked despite academic studies

showing that students admitted to both I lar
vard and Princeton would choose Harvard
70 percent of the time. Sorting students by
where they actually choose to go to school,
rather than where some profit-seeking mag
azine thinks they should go is the most
meaningful way to rank colleges. Important
ly. it prevents colleges from manipulating
statistics to move up the rankings. Any in
centives that colleges can use to increase
their cross-admit yield, like better financial
aid offers and merit-scholarship programs,
will only benefit students. Instead, the U.S.
News rankings set up a perverse system
where schools like Washington University
in St. Louis will waitlist thousands of good
students in order to lower their acceptance
rate.
It has reached a point where the rankings
have a pernicious effect on the behavior of
schools themselves. Instead of moving to a
more transparent, nonbinding early action
program, or even better, eliminating early
admissions entirely, the rankings incentivize
schools to hold onto their early decision pro
grams and use the waitlist as much as possi
ble. There is even speculation that Stanford's
proposal to increase its class size by 2(H) stu
dents is motivated by Princeton's recent class
size increase, as well as Yale and Chicago's
plans to do so.
Competition and transparency are good
for college admissions. We are, unfortunately,
working under a system that does not allow
the efficiencies of the market to work. A lack
of access to information that really matters,
not "alumni giving rate" and "faculty re
sources" hinders the ability to rank colleges
effectively and most importantly, for high
school students to make the right decision
about their educational future.

Stuart Baimel thinks that the preseason college
football rankings are as distorted as the U.S.
News rankings. Oklahoma and C lemson are
ranked five places too high every year. Send
him your pick for Most Overrated to shaimel
"at" Stanford.edit.

This column originally ran on Aug. 28, 2008.
August has been a summer of circuses.
We first had the circus of the
Olympics.Then we have the circus of
the Democratic National Convention, a glo
rified extended commercial that the media
pretends actually matters. But at least those
two happen once every four years.The high
er-education community has to endure annu
al unveilings of the controversial U.S. News
& World Report rankings of undergraduate
universities. Who went down? Who went up?
Which schools "goosed" the rankings? Like
it or not, admissions offices nationwide must
hang on every decimal point, as they know an
improved ranking can drive thousands more
applicants to their school. Just witness the
boon to Penn's acceptance rate after the
school cracked the top five.
It is indeed a shame that these rankings
matter. Besides the obvious conflict-of-inter
est (the conflicting priorities of higher educa
tion and a magazine trying to sell as many is
sues as possible), the U.S. News & World Re
port methodology is grossly flawed. Instead
of measuring outcomes, the report focuses on
incoming statistics — SAT ranges, incoming
students in the top 10 percent of their high
school class and so on."Freshman retention"
weighs heavily, so schools that have difficult
first-year programs like Caltech and MIT
suffer.
The magazine also uses "alumni giving
rate" as part of its methodology; it is unclear
how an alum from the Class of 1980 writing a
$ 1 (K) check has anything to do with the qual
ity of the school in 2008. So although Stan
ford tends to raise twice as much money an
nually as Princeton, they trail in that catego
ry due to the higher number of alumni at
Princeton who donate. Indeed, the rankings
have not been kind to the Farm. Stanford has
trailed behind Harvard, Yale and Princeton
for the past decade — mostly because of the
lower admissions standards for athletes, in
my opinion.
Rankings have become quite an industry.

The undergraduate rankings issue is U.S.
News's top-selling. Forbes, Newsweek and
other publications have created their own
rankings to capitalize on the rankings bo
nanza as undergraduate admissions be
come ever-more competitive. Forbes' at
tempt was particularly weak; a full 25 per
cent of the methodology was based on rate
myprofessors.com. The use of that site cre
ates debilitating selection bias issues that
should — although they won't — de-legit
imize Forbes' rankings entirely. I do not
know a single person who has ever used
that Web site. U.S. News, for its part, does
not make its "Common Data Set," which all
universities submit, publicly available, hin
dering transparency.
Most critics of the U.S. News rankings
argue that it is impossible to rank schools at
all. I disagree. A few years ago, the Wall Street
Journal did a study of professional school
placement. They ranked undergraduate insti
tutions by how many students (as a percent
age of their class) got into lop professional
schools. While professional-school place
ment is only one measure of educational out
come. it is decidedly an important one and
shows which universities professional
schools prefer. But very little data of that sort
is publicly available. More studies that meas
ure placement and outcome would rank
schools far more accurately than the dreamt
up, always-changing methodology of U.S.
News.
"Cross-admits" remain the holy grail of
undergraduate admissions statistics, and the
data most useful to those attempting to rank
schools. For the past several years, Princeton
was top-ranked despite academic studies

showing that students admitted to both I lar
vard and Princeton would choose Harvard
70 percent of the time. Sorting students by
where they actually choose to go to school,
rather than where some profit-seeking mag
azine thinks they should go is the most
meaningful way to rank colleges. Important
ly. it prevents colleges from manipulating
statistics to move up the rankings. Any in
centives that colleges can use to increase
their cross-admit yield, like better financial
aid offers and merit-scholarship programs,
will only benefit students. Instead, the U.S.
News rankings set up a perverse system
where schools like Washington University
in St. Louis will waitlist thousands of good
students in order to lower their acceptance
rate.
It has reached a point where the rankings
have a pernicious effect on the behavior of
schools themselves. Instead of moving to a
more transparent, nonbinding early action
program, or even better, eliminating early
admissions entirely, the rankings incentivize
schools to hold onto their early decision pro
grams and use the waitlist as much as possi
ble. There is even speculation that Stanford's
proposal to increase its class size by 2(H) stu
dents is motivated by Princeton's recent class
size increase, as well as Yale and Chicago's
plans to do so.
Competition and transparency are good
for college admissions. We are, unfortunately,
working under a system that does not allow
the efficiencies of the market to work. A lack
of access to information that really matters,
not "alumni giving rate" and "faculty re
sources" hinders the ability to rank colleges
effectively and most importantly, for high
school students to make the right decision
about their educational future.

Stuart Baimel thinks that the preseason college
football rankings are as distorted as the U.S.
News rankings. Oklahoma and C lemson are
ranked five places too high every year. Send
him your pick for Most Overrated to shaimel
"at" Stanford.edit.

This column originally ran on Aug. 28, 2008.
August has been a summer of circuses.
We first had the circus of the
Olympics.Then we have the circus of
the Democratic National Convention, a glo
rified extended commercial that the media
pretends actually matters. But at least those
two happen once every four years.The high
er-education community has to endure annu
al unveilings of the controversial U.S. News
& World Report rankings of undergraduate
universities. Who went down? Who went up?
Which schools "goosed" the rankings? Like
it or not, admissions offices nationwide must
hang on every decimal point, as they know an
improved ranking can drive thousands more
applicants to their school. Just witness the
boon to Penn's acceptance rate after the
school cracked the top five.
It is indeed a shame that these rankings
matter. Besides the obvious conflict-of-inter
est (the conflicting priorities of higher educa
tion and a magazine trying to sell as many is
sues as possible), the U.S. News & World Re
port methodology is grossly flawed. Instead
of measuring outcomes, the report focuses on
incoming statistics — SAT ranges, incoming
students in the top 10 percent of their high
school class and so on."Freshman retention"
weighs heavily, so schools that have difficult
first-year programs like Caltech and MIT
suffer.
The magazine also uses "alumni giving
rate" as part of its methodology; it is unclear
how an alum from the Class of 1980 writing a
$ 1 (K) check has anything to do with the qual
ity of the school in 2008. So although Stan
ford tends to raise twice as much money an
nually as Princeton, they trail in that catego
ry due to the higher number of alumni at
Princeton who donate. Indeed, the rankings
have not been kind to the Farm. Stanford has
trailed behind Harvard, Yale and Princeton
for the past decade — mostly because of the
lower admissions standards for athletes, in
my opinion.
Rankings have become quite an industry.

The undergraduate rankings issue is U.S.
News's top-selling. Forbes, Newsweek and
other publications have created their own
rankings to capitalize on the rankings bo
nanza as undergraduate admissions be
come ever-more competitive. Forbes' at
tempt was particularly weak; a full 25 per
cent of the methodology was based on rate
myprofessors.com. The use of that site cre
ates debilitating selection bias issues that
should — although they won't — de-legit
imize Forbes' rankings entirely. I do not
know a single person who has ever used
that Web site. U.S. News, for its part, does
not make its "Common Data Set," which all
universities submit, publicly available, hin
dering transparency.
Most critics of the U.S. News rankings
argue that it is impossible to rank schools at
all. I disagree. A few years ago, the Wall Street
Journal did a study of professional school
placement. They ranked undergraduate insti
tutions by how many students (as a percent
age of their class) got into lop professional
schools. While professional-school place
ment is only one measure of educational out
come. it is decidedly an important one and
shows which universities professional
schools prefer. But very little data of that sort
is publicly available. More studies that meas
ure placement and outcome would rank
schools far more accurately than the dreamt
up, always-changing methodology of U.S.
News.
"Cross-admits" remain the holy grail of
undergraduate admissions statistics, and the
data most useful to those attempting to rank
schools. For the past several years, Princeton
was top-ranked despite academic studies

showing that students admitted to both I lar
vard and Princeton would choose Harvard
70 percent of the time. Sorting students by
where they actually choose to go to school,
rather than where some profit-seeking mag
azine thinks they should go is the most
meaningful way to rank colleges. Important
ly. it prevents colleges from manipulating
statistics to move up the rankings. Any in
centives that colleges can use to increase
their cross-admit yield, like better financial
aid offers and merit-scholarship programs,
will only benefit students. Instead, the U.S.
News rankings set up a perverse system
where schools like Washington University
in St. Louis will waitlist thousands of good
students in order to lower their acceptance
rate.
It has reached a point where the rankings
have a pernicious effect on the behavior of
schools themselves. Instead of moving to a
more transparent, nonbinding early action
program, or even better, eliminating early
admissions entirely, the rankings incentivize
schools to hold onto their early decision pro
grams and use the waitlist as much as possi
ble. There is even speculation that Stanford's
proposal to increase its class size by 2(H) stu
dents is motivated by Princeton's recent class
size increase, as well as Yale and Chicago's
plans to do so.
Competition and transparency are good
for college admissions. We are, unfortunately,
working under a system that does not allow
the efficiencies of the market to work. A lack
of access to information that really matters,
not "alumni giving rate" and "faculty re
sources" hinders the ability to rank colleges
effectively and most importantly, for high
school students to make the right decision
about their educational future.

Stuart Baimel thinks that the preseason college
football rankings are as distorted as the U.S.
News rankings. Oklahoma and C lemson are
ranked five places too high every year. Send
him your pick for Most Overrated to shaimel
"at" Stanford.edit.

